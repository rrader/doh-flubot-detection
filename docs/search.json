[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2023-03-08-flubot-multiple-nodes",
    "section": "",
    "text": "1 Overview\nIn this experiment we are analyzing the proposed method of detection DNS-over-HTTPS Exfiltration traffic."
  },
  {
    "objectID": "index.html#flubot-malware",
    "href": "index.html#flubot-malware",
    "title": "2023-03-08-flubot-multiple-nodes",
    "section": "1.1 FluBot malware",
    "text": "1.1 FluBot malware\nWe are using the sample of FluBot malware - Android malware which performs exfiltration of data using DoH, (Figure 1.1). It was active during 2021 and 2022, in the\nSample of FluBot 5.2 was downloaded from the vx-underground. In the version 5.0 DoH tunneling feature was integrated to the FluBot.\nIn the May 2022 the FluBot infrastracture was taken down by then EuroPol. However, we use it as a real-world malware sample to validate the DoH tunneling/exfiltration detection algorithm.\nReport of F5.com on the FluBot malware can be found here.\n\n\n\nFigure 1.1: Flubot masks as a flash player"
  },
  {
    "objectID": "index.html#proposed-method",
    "href": "index.html#proposed-method",
    "title": "2023-03-08-flubot-multiple-nodes",
    "section": "1.2 Proposed method",
    "text": "1.2 Proposed method\nThe proposed method is based on the two steps:\n\nClassify network flows as a DoH or non-DoH, with a classifier built on the Jeřábek et al. (2022) dataset (https://zenodo.org/record/6024914#.ZAcru-xBzaV)\nApply the Netflow-based DGA detection algorithm Grill et al. (2015) to detect infected machines\n\nMachine-learning classification of DoH traffic allows us to apply the algorithm to DoH traffic, originally designed for the plaintext DNS."
  },
  {
    "objectID": "index.html#doh-traffic-classification",
    "href": "index.html#doh-traffic-classification",
    "title": "2023-03-08-flubot-multiple-nodes",
    "section": "1.3 DoH traffic classification",
    "text": "1.3 DoH traffic classification\nDoH traffic uses using standard HTTPS 443 port, which blends DNS packets within the generic HTTPS traffic. Netflow/IPFIX data of the DoH traffic does not contain direct features which could help to distinguish DNS traffic from other HTTPS communications. In this experiment we build a classifier on the Jeřábek et al. (2022) dataset using Logistic Regression, Random Forest and the Histogram-based Gradient Boosting (Section 7.1), and selecting the models which are the most robust to the different network environment (Section 8.1)."
  },
  {
    "objectID": "index.html#infected-network-traffic-recording",
    "href": "index.html#infected-network-traffic-recording",
    "title": "2023-03-08-flubot-multiple-nodes",
    "section": "1.4 Infected network traffic recording",
    "text": "1.4 Infected network traffic recording\nFor the experiment, we modelled the malware detection algorithm deployment into the production network. For that we deployed the sandbox network environment into the Proxmox server with 4 machines: 3 Android x86 emulators and 1 Kali Linux instance (see Chapter 2), behind the OpenWRT router provisioned with the IPFIXprobe tool for recording traffic on the network border.\nOne of the Android machines was infected by the FluBot malware during the experiment, the traffic was collected and saved to the CSV files (see Chapter 3) Figure 1.2 .\n\n\n\nFigure 1.2: Screenshot"
  },
  {
    "objectID": "index.html#model-training",
    "href": "index.html#model-training",
    "title": "2023-03-08-flubot-multiple-nodes",
    "section": "1.5 Model training",
    "text": "1.5 Model training\nModel training includes data preparation in two steps (cached processing and preprocessing with hyperparameters) (see Chapter 4).\n\n\n\nFigure 1.3: Model training overview"
  },
  {
    "objectID": "index.html#malware-detection",
    "href": "index.html#malware-detection",
    "title": "2023-03-08-flubot-multiple-nodes",
    "section": "1.6 Malware detection",
    "text": "1.6 Malware detection\nTo detect the malicious behavior of the machine, we propose to use the ratio of DNS requests and contacted IP addresses for every host in the local network described in the Grill et al. (2015). The paper was focused on DGA-based malware detection. However, the approach should be used to detect DNS data exfiltration, too, since the behavior pattern is similar (see Chapter 10).\n\n\n\nFigure 1.4: Malware detection overview\n\n\n\n\n\n\nGrill, Martin, Ivan Nikolaev, Veronica Valeros, and Martin Rehak. 2015. “Detecting DGA Malware Using NetFlow.” In 2015 IFIP/IEEE International Symposium on Integrated Network Management (IM), 1304–9. https://doi.org/10.1109/INM.2015.7140486.\n\n\nJeřábek, Kamil, Karel Hynek, Tomáš Čejka, and Ondřej Ryšavý. 2022. “Collection of Datasets with DNS over HTTPS Traffic.” Data in Brief 42 (June): 108310. https://doi.org/10.1016/j.dib.2022.108310."
  },
  {
    "objectID": "collect-data.html",
    "href": "collect-data.html",
    "title": "Experiment 1: “Clean room” with multiple Androids",
    "section": "",
    "text": "In this part we collect data from 3 Android devices on the sandbox network"
  },
  {
    "objectID": "experiment.html",
    "href": "experiment.html",
    "title": "2  Design and log of experiment 1",
    "section": "",
    "text": "3 Experiment\nExperiment is conducted in two parts (see Figure 3.1):"
  },
  {
    "objectID": "experiment.html#openwrt-router-configuration",
    "href": "experiment.html#openwrt-router-configuration",
    "title": "2  Design and log of experiment 1",
    "section": "2.1 OpenWRT router configuration",
    "text": "2.1 OpenWRT router configuration\nOpenWrt is provided with IPFIXprobe package installed from https://github.com/CESNET/Nemea-OpenWRT with\n\npcap support\nNEMEA unirec support\npstats plugin collects 100 first packets (30 by default) PSTATS_MAXELEMCOUNT = 100: process/pstats.hpp:# define PSTATS_MAXELEMCOUNT 100"
  },
  {
    "objectID": "experiment.html#infection-of-android-with-flubot",
    "href": "experiment.html#infection-of-android-with-flubot",
    "title": "2  Design and log of experiment 1",
    "section": "2.2 Infection of Android with FluBot",
    "text": "2.2 Infection of Android with FluBot\nWe install flubot to 192.168.2.217 Android using sample from vx-underground.\ninfect.sh:\napk=flubot.apk\n\n# disable Play Protect\nadb -s 192.168.2.217:5555 root\nadb -s 192.168.2.217:5555 shell settings put global package_verifier_user_consent -1\n\nadb -s 192.168.2.217:5555 shell settings put global verifier_verify_adb_installs 0\n\npkg=$(aapt dump badging $apk|awk -F\" \" '/package/ {print $2}'|awk -F\"'\" '/name=/ {print $2}')\nact=$(aapt dump badging $apk|awk -F\" \" '/launchable-activity/ {print $2}'|awk -F\"'\" '/name=/ {print $2}')\n\nadb -s 192.168.2.217:5555 shell pm unhide $pkg\nadb -s 192.168.2.217:5555 install $apk\nadb -s 192.168.2.217:5555 shell pm unhide $pkg\nadb -s 192.168.2.217:5555 shell pm enable $pkg\n\nadb -s 192.168.2.217:5555 shell am start -n $pkg/$act\nadb -s 192.168.2.217:5555 shell pm enable $pkg\n\nadb -s 192.168.2.217:5555 shell settings put secure enabled_accessibility_services com.tencent.mobileqq/Flash:com.tencent.mobileqq/com.tencent.mobileqq.pcdf91408\n\nsleep 2\n\nadb -s 192.168.2.217:5555 shell am start -n $pkg/$act\nThen we need to allow “Flash Player” (flubot) permission for accessibility services and machine is infected."
  },
  {
    "objectID": "experiment.html#generate-benign-traffic",
    "href": "experiment.html#generate-benign-traffic",
    "title": "2  Design and log of experiment 1",
    "section": "2.3 Generate benign traffic",
    "text": "2.3 Generate benign traffic\nOn 192.168.2.149 and 192.168.2.249 machines we generate benign traffic. Both ot them are going to browse top 500 domains from the moz.com.\nTo do that, we install the latest Chrome (&gt;=83 with DoH support). Chrome 111.\nOn 192.168.2.149 we configure DoH (see Figure 2.2).\n\n\n\nFigure 2.2: Chrome configured to use DoH\n\n\nChrome installation:\nadb shell settings put global verifier_verify_adb_installs 0\nadb install chrome.apk\nScript traffic.py (Section B.1) randomly navigates both machines to different websites from the top 500 list. Once in a while it changes the DoH provider on the 192.168.2.149 machine from the predefined list:\n\nhttps://dns.google/dns-query\nhttps://cloudflare-dns.com/dns-query\nhttps://dns.quad9.net/dns-query\nhttps://unfiltered.adguard-dns.com/dns-query\nhttps://doh.cleanbrowsing.org/doh/security-filter/\nhttps://freedns.controld.com/p0"
  },
  {
    "objectID": "experiment.html#record-traffic",
    "href": "experiment.html#record-traffic",
    "title": "2  Design and log of experiment 1",
    "section": "2.4 Record traffic",
    "text": "2.4 Record traffic\n\nStart all instances Figure 1.2:\nStart ipfixprobe on exp-outer-router.local:\n\nprobe.sh:\nset -x\nservice dnsmasq stop\nservice dnsmasq start\nipfixprobe -i 'pcap;ifc=eth1' -p \"pstats\" -p \"phists\" -p \"tls\" -o 'unirec;i=t:4739:timeout=NO_WAIT:buffer=off:autoflush=off;p=(pstats,phists,tls);v'\n\nStart logger (IPFIX collector from the ipfixprobe) conda.local:\n\nlogger.sh:\n/usr/bin/nemea/logger -i \"t:192.168.1.141:4739\" -t | tee out.csv\nCaptured traffic is saved to the CSV file using the logger tool from the NEMEA framework Figure 2.3.\n\n\n\nFigure 2.3: Capture"
  },
  {
    "objectID": "experiment.html#log",
    "href": "experiment.html#log",
    "title": "2  Design and log of experiment 1",
    "section": "3.1 Log",
    "text": "3.1 Log\n\n\n\n\n\n\n\n\n\n\nTimestamp\nTime from start (minutes)\nDuration\nLog\nSave\n\n\n\n\n4:07:16 PM\n0\n\nStart data collection (ipfixprobe and logger)\n\n\n\n4:08:00 PM\n1\n1\nStarted traffic.py\n\n\n\n4:49:21 PM\n42\n41\nStop data collection\nsave result to v5_1.csv\n\n\n4:50:21 PM\n43\n1\nStart data collection again (ipfixprobe and logger)\n\n\n\n5:50:22 PM\n103\n60\nStarted infect.sh\n\n\n\n6:50:31 PM\n163\n60\nStop data collection\nsave result to v5_2.csv"
  },
  {
    "objectID": "experiment.html#artifacts",
    "href": "experiment.html#artifacts",
    "title": "2  Design and log of experiment 1",
    "section": "3.2 Artifacts",
    "text": "3.2 Artifacts\n\nB1 part of traffic saved into v5_1.csv\nB2 part of traffic saved into v5_2.csv\ntraffic.py log is saved in v5_exp.log\nHigh-level log with timestamps of experiment is saved in v5_log.csv"
  },
  {
    "objectID": "notebooks/analyze-collected-dataset.html#multi-node-experiment-with-flubot-and-benign-doh",
    "href": "notebooks/analyze-collected-dataset.html#multi-node-experiment-with-flubot-and-benign-doh",
    "title": "3  Analyze generated dataset",
    "section": "3.1 Multi-node experiment with FluBot and benign DoH",
    "text": "3.1 Multi-node experiment with FluBot and benign DoH\nTraffic stored in v5_1.csv (B1) and v5_2.csv (B2) (Figure 3.1).\nLet’s convert it to feather for faster processing, and get a bird view on the data.\n\ndf = pd.read_csv(basepath + \"datasets/flubot20230323/data/v5_1.csv\")\ndf.to_feather(\"cached/v5_1.ft\")\ndf = pd.read_csv(basepath + \"datasets/flubot20230323/data/v5_2.csv\")\ndf.to_feather(\"cached/v5_2.ft\")\n\n\nimport dateutil.parser\n\ndf1 = pd.read_feather(\"cached/v5_1.ft\")\ndf1[\"time TIME_FIRST\"] = df1[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n\n\nlen(df1)\n\n22640\n\n\n\ndf2 = pd.read_feather(\"cached/v5_2.ft\")\ndf2[\"time TIME_FIRST\"] = df2[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n\n\nlen(df2)\n\n181357\n\n\n\ndf.info()\n\n&lt;class 'modin.pandas.dataframe.DataFrame'&gt;\nRangeIndex: 181357 entries, 0 to 181356\nData columns (total 29 columns):\n #   Column                    Non-Null Count   Dtype \n---  ------------------------  ---------------  ----- \n 0   ipaddr DST_IP             181357 non-null  object\n 1   ipaddr SRC_IP             181357 non-null  object\n 2   uint64 BYTES              181357 non-null  int64\n 3   uint64 BYTES_REV          181357 non-null  int64\n 4   uint64 LINK_BIT_FIELD     181357 non-null  int64\n 5   time TIME_FIRST           181357 non-null  object\n 6   time TIME_LAST            181357 non-null  object\n 7   macaddr DST_MAC           181357 non-null  object\n 8   macaddr SRC_MAC           181357 non-null  object\n 9   uint32 PACKETS            181357 non-null  int64\n 10  uint32 PACKETS_REV        181357 non-null  int64\n 11  uint16 DST_PORT           181357 non-null  int64\n 12  uint16 SRC_PORT           181357 non-null  int64\n 13  uint16 TLS_VERSION        181357 non-null  int64\n 14  uint8 DIR_BIT_FIELD       181357 non-null  int64\n 15  uint8 PROTOCOL            181357 non-null  int64\n 16  uint8 TCP_FLAGS           181357 non-null  int64\n 17  uint8 TCP_FLAGS_REV       181357 non-null  int64\n 18  int8* PPI_PKT_DIRECTIONS  181357 non-null  object\n 19  uint8* PPI_PKT_FLAGS      181357 non-null  object\n 20  string TLS_ALPN           88438 non-null   object\n 21  bytes TLS_JA3             157307 non-null  object\n 22  string TLS_SNI            156861 non-null  object\n 23  uint16* PPI_PKT_LENGTHS   181357 non-null  object\n 24  uint32* D_PHISTS_IPT      181357 non-null  object\n 25  uint32* D_PHISTS_SIZES    181357 non-null  object\n 26  uint32* S_PHISTS_IPT      181357 non-null  object\n 27  uint32* S_PHISTS_SIZES    181357 non-null  object\n 28  time* PPI_PKT_TIMES       181357 non-null  object\ndtypes: object(17), int64(12)\nmemory usage: 40.1 MB\n\n\n\nlocal_df1 = df1[df1[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\nlocal_df2 = df2[df2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\nlen(local_df1), len(local_df2)\n\n(18218, 173067)\n\n\n\nlocal_ips2 = local_df2[\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips2\n\n['192.168.2.249',\n '192.168.2.149',\n '192.168.2.42',\n '192.168.2.217',\n '192.168.1.1']\n\n\n\nlocal_ips1 = local_df1[\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips = list(set(local_ips1 + local_ips2))\nlocal_ips\n\n['192.168.2.42',\n '192.168.2.217',\n '192.168.1.1',\n '192.168.2.149',\n '192.168.2.249']"
  },
  {
    "objectID": "notebooks/analyze-collected-dataset.html#b1-period-vs-b2-period",
    "href": "notebooks/analyze-collected-dataset.html#b1-period-vs-b2-period",
    "title": "3  Analyze generated dataset",
    "section": "3.2 B1 period vs B2 period",
    "text": "3.2 B1 period vs B2 period\n\nfrom matplotlib import pyplot as plt\n\nd = {\n    'B1': local_df1[\"ipaddr SRC_IP\"].value_counts().to_dict(),\n    'B2': local_df2[\"ipaddr SRC_IP\"].value_counts().to_dict(),\n}\n\npd.DataFrame(d).plot(kind='bar', log=True)\nplt.show()"
  },
  {
    "objectID": "notebooks/analyze-collected-dataset.html#plaintext-dns-vs-whole-https-traffic",
    "href": "notebooks/analyze-collected-dataset.html#plaintext-dns-vs-whole-https-traffic",
    "title": "3  Analyze generated dataset",
    "section": "3.3 Plaintext DNS vs whole HTTPS traffic",
    "text": "3.3 Plaintext DNS vs whole HTTPS traffic\n\nfrom matplotlib import pyplot as plt\n\nfig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n\nfor ax, df in zip(axs, [local_df1, local_df2]):\n    d = {\n        '443': df[df[\"uint16 DST_PORT\"] == 443][\"ipaddr SRC_IP\"].value_counts().to_dict(),\n        '53': df[df[\"uint16 DST_PORT\"] == 53][\"ipaddr SRC_IP\"].value_counts().to_dict(),\n    }\n    pd.DataFrame(d).plot(kind='bar', log=True, ax=ax)\n\nplt.show()"
  },
  {
    "objectID": "notebooks/analyze-collected-dataset.html#timeline-of-the-traffic",
    "href": "notebooks/analyze-collected-dataset.html#timeline-of-the-traffic",
    "title": "3  Analyze generated dataset",
    "section": "3.4 Timeline of the traffic",
    "text": "3.4 Timeline of the traffic\n\nmin_ts = local_df1[\"time TIME_FIRST\"].min()\nmin_ts_b2 = local_df2[\"time TIME_FIRST\"].min()\nmin_ts, min_ts_b2\n\n(Timestamp('2023-03-23 14:07:18.810756'),\n Timestamp('2023-03-23 14:50:04.318641'))\n\n\n\nf = '2min'\n\ndef plot_packets_by_port(local_df, host, dst_port):\n    a1_t = local_df[local_df[\"ipaddr SRC_IP\"] == host].set_index([\"time TIME_FIRST\"])\n\n    a1_t53 = a1_t[a1_t[\"uint16 DST_PORT\"] == dst_port][[\"uint32 PACKETS\", \"uint32 PACKETS_REV\"]]\n    a1_t53[\"PACKETS\"] = a1_t53[\"uint32 PACKETS\"] + a1_t53[\"uint32 PACKETS_REV\"]\n    df_resample = a1_t53.groupby(pd.Grouper(freq=f, origin=min_ts)).sum()\n    return df_resample\n\n\nlocal_df = pd.concat([local_df1, local_df2]).reset_index(drop=True)\n\n\ndf_plots = {h: plot_packets_by_port(local_df, h, 53) for h in local_ips}\n\n\npd.DataFrame({\n    h: df[\"PACKETS\"].to_dict()\n    for h, df in df_plots.items()\n    if len(df) &gt; 0\n}).plot()\n\nplt.axvline(x=min_ts_b2)\n\nplt.title(\"Plaintext DNS packets\")\nplt.plot()\n\n[]\n\n\n\n\n\n\ndf_plots = {h: plot_packets_by_port(local_df, h, 443) for h in local_ips}\n\n\npd.DataFrame({\n    h: df[\"PACKETS\"].to_dict()\n    for h, df in df_plots.items()\n    if len(df) &gt; 0\n}).plot(logy=True)\n\nplt.axvline(x=min_ts_b2)\n\nplt.title(\"HTTPS packets\")\nplt.plot()\n\n[]"
  },
  {
    "objectID": "notebooks/analyze-collected-dataset.html#lets-filter-out-the-benign-doh-traffic",
    "href": "notebooks/analyze-collected-dataset.html#lets-filter-out-the-benign-doh-traffic",
    "title": "3  Analyze generated dataset",
    "section": "3.5 Let’s filter out the benign DoH traffic",
    "text": "3.5 Let’s filter out the benign DoH traffic\n\ndns_providers = [\n    \"dns.google\",\n    \"cloudflare-dns.com\",\n    \"dns.quad9.net\",\n    \"adguard-dns.com\",\n    \"doh.cleanbrowsing.org\",\n    \"freedns.controld.com\",\n]\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\ndf = pd.concat([\n    pd.read_feather(\"cached/v5_1.ft\"), pd.read_feather(\"cached/v5_2.ft\")\n])\nsome_doh = df[df[\"ipaddr SRC_IP\"] == \"192.168.2.149\"]\n\n\ndef filter_doh(sni):\n    if not sni:\n        return False\n    return any(pat in sni for pat in dns_providers)\n\nsome_doh_https = some_doh[some_doh[\"uint16 DST_PORT\"] == 443]\ndoh_only = some_doh_https[some_doh_https[\"string TLS_SNI\"].apply(filter_doh, axis=1)]\nlen(doh_only)\n\n624\n\n\n\ndoh_ips = pd.DataFrame({\"ip\": doh_only[\"ipaddr DST_IP\"].unique()})\ndoh_ips\n\n\n\n\n\n\n\n\nip\n\n\n\n\n0\n104.16.249.249\n\n\n1\n104.16.248.249\n\n\n2\n94.140.14.141\n\n\n3\n94.140.14.140\n\n\n4\n185.228.168.168\n\n\n5\n8.8.8.8\n\n\n6\n8.8.4.4\n\n\n7\n149.112.112.112\n\n\n8\n76.76.2.11\n\n\n9\n185.228.168.10\n\n\n10\n9.9.9.9\n\n\n\n\n\n\n\n\ndoh_ips.to_csv(basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\", index=False)"
  },
  {
    "objectID": "train-model.html",
    "href": "train-model.html",
    "title": "Train DoH detection models",
    "section": "",
    "text": "In this part we build a model on the Jeřábek et al. (2022) dataset, then select a model using test set built partly from training data, partly from the data collected from benign instances of sandbox network environment.\n\n\n\n\nJeřábek, Kamil, Karel Hynek, Tomáš Čejka, and Ondřej Ryšavý. 2022. “Collection of Datasets with DNS over HTTPS Traffic.” Data in Brief 42 (June): 108310. https://doi.org/10.1016/j.dib.2022.108310."
  },
  {
    "objectID": "train-model-plan.html#training-data",
    "href": "train-model-plan.html#training-data",
    "title": "4  Training models",
    "section": "4.1 Training data",
    "text": "4.1 Training data\nModel training is done on the DoH traffic collection described in Jeřábek et al. (2022)\n\nhttps://zenodo.org/record/6024914#.ZAcru-xBzaV\n\nThis dataset contains a large collection of generated DoH data from Chrome and Firefox browsers using GET and POST methods on multiple DoH public resolvers."
  },
  {
    "objectID": "train-model-plan.html#training-process-overview",
    "href": "train-model-plan.html#training-process-overview",
    "title": "4  Training models",
    "section": "4.2 Training process overview",
    "text": "4.2 Training process overview\nModels training is done in the following steps:\n\nCacheable processing\n\nFilter HTTPS traffic\nParse packet lengths and timestamps array fields\nSplit packet lengths and timestamps arrays by outgoing and incoming directions\nCalculate inter-packet duration from the timesamps\nLabel flows (boolean IsDoH field)\n\nPreProcessing\n\nCalculate stats (mean, variance, stddev) on\n\nPacket lengths (all, incoming, outgoing)\nInter-packet duration (all, incoming, outgoing)\n\nNormalization of the feature fields\nSplit features and label columns\n\n\nFeature fields (1 and -1 represent outgoing and incoming packets respectively):\n\nuint16* PPI_PKT_LENGTHS_stddev\nuint16* PPI_PKT_LENGTHS_mean\nuint16* PPI_PKT_LENGTHS_var\nuint16* PPI_PKT_LENGTHS_1_stddev\nuint16* PPI_PKT_LENGTHS_1_mean\nuint16* PPI_PKT_LENGTHS_1_var\nuint16* PPI_PKT_LENGTHS_-1_stddev\nuint16* PPI_PKT_LENGTHS_-1_mean\nuint16* PPI_PKT_LENGTHS_-1_var\nPPI_PKT_INTERVALS_stddev\nPPI_PKT_INTERVALS_mean\nPPI_PKT_INTERVALS_var\nPPI_PKT_INTERVALS_1_stddev\nPPI_PKT_INTERVALS_1_mean\nPPI_PKT_INTERVALS_1_var\nPPI_PKT_INTERVALS-1_stddev\nPPI_PKT_INTERVALS-1_mean\nPPI_PKT_INTERVALS-1_var"
  },
  {
    "objectID": "train-model-plan.html#data-preparation-diagrams",
    "href": "train-model-plan.html#data-preparation-diagrams",
    "title": "4  Training models",
    "section": "4.3 Data preparation diagrams",
    "text": "4.3 Data preparation diagrams\nData preparation is done in two steps: - Cacheable processing (non-parametric processing) Figure 4.1 - Preprocessing (applies hyperparameters) Figure 4.2\n\n\n\nFigure 4.1: Cachable processing\n\n\n\n\n\nFigure 4.2: Preprocessing"
  },
  {
    "objectID": "train-model-plan.html#hyperparameters-selection",
    "href": "train-model-plan.html#hyperparameters-selection",
    "title": "4  Training models",
    "section": "4.4 Hyperparameters selection",
    "text": "4.4 Hyperparameters selection\n\nNormalization\n\nThe hypothesis is that the normalization of statistical features separately on training and test (production) set will allow us reduce the concept drift in case these datasets were recorded in different networks (see Section 10.1).\n\nSkip first packets\n\n(TODO: reference)\nSkipping first few packets should reduce the importrance of TLS handshake which may be very similar for DoH and non-DoH HTTPS traffic, and focus the detection on the traffic content.\nOther technique which is tested in this experiment is applying weights on the features, instead of just skipping them (in this case skipping the packet equal to 0.0 weight).\nWeight for N packets is calculated linearly increasing:\n\nfrom \\(0.1\\) for the 1 packet\nto \\(1.0\\) for N+1’s packet.\n\nFor example, for skip=1, weight=2 and length of packets array=7 the weights will be the following:\n\n\n\nid\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\naction\nskip\nweight\nweight\n-\n-\n-\n-\n\n\nweight\n0.\n0.1\n0.55\n1.\n1.\n1.\n1.\n\n\n\nThen we calculate the weighted mean, weighted variance and weighted standard deviation for the features:\n\nAll packet lengths: uint16* PPI_PKT_LENGTHS\nOutgoing packet lengths: uint16* PPI_PKT_LENGTHS_1\nIncoming packet lengths: uint16* PPI_PKT_LENGTHS_-1\nAll inter-packet durations: PPI_PKT_INTERVALS\nOutgoing inter-packet durations: PPI_PKT_INTERVALS_1\nIncoming inter-packet durations: PPI_PKT_INTERVALS-1\n\nThis way we have following hyperparameters represented as a tuple of (norm, A, B, C, D):\n\nnormalization (on/off)\nskip A first packets for inter-packet duration statistics\nskip B first packets for packet sizes statistics\napply weight on C first packets for inter-packet duration statistics\napply weight on D first packets for sizes statistics\n\n\n\n\n\nJeřábek, Kamil, Karel Hynek, Tomáš Čejka, and Ondřej Ryšavý. 2022. “Collection of Datasets with DNS over HTTPS Traffic.” Data in Brief 42 (June): 108310. https://doi.org/10.1016/j.dib.2022.108310."
  },
  {
    "objectID": "notebooks/sample-Jerabek2022-dataset.html#real-world-traffic-sampling",
    "href": "notebooks/sample-Jerabek2022-dataset.html#real-world-traffic-sampling",
    "title": "5  Sample learning dataset",
    "section": "5.1 Real world traffic sampling",
    "text": "5.1 Real world traffic sampling\n\n5.1.1 Sampled to 10% of DoH and 100% of HTTPS traffic\nDoH and HTTPS are sampled separately, because in the resulting dataset we may want to have different ratio of DoH and HTTPS traffic to get more benign samples.\n\nprefix = basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100/\"\nprint(\"files:\", len(list(enum_csv(prefix))))\n\nfor i, f in enumerate(f for f in enum_csv(prefix) if ('DoH-Real-World' in str(f)) and ('DoH' in os.path.basename(f))):\n    if i == 0:\n        kwargs = {\"mode\": \"w\", \"header\": True}\n    else:\n        kwargs = {\"mode\": \"a\", \"header\": False}\n\n    df = pd.read_csv(f)\n    df_sampled = df.sample(frac=0.1, random_state=42)\n    print(os.path.basename(f), \"size:\", len(df), \"; sampled size:\", len(df_sampled))\n    df_sampled.to_csv(\n        basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100-sample-01-real-world-doh.csv\", \n        **kwargs\n    )\n\nfiles: 81\nDoH-01082021-48h.pcapng.trapcap.csv size: 1108565 ; sampled size: 110856\n\n\nDoH-03082021-48h.pcapng.trapcap.csv size: 1011530 ; sampled size: 101153\n\n\nDoH-06102021-48h.pcapng.trapcap.csv size: 3588446 ; sampled size: 358845\n\n\nDoH-08102021-48h.pcapng.trapcap.csv size: 1392097 ; sampled size: 139210\n\n\nDoH-13072021-48h.pcapng.trapcap.csv size: 1272538 ; sampled size: 127254\n\n\nDoH-15072021-48h.pcapng.trapcap.csv size: 1079644 ; sampled size: 107964\n\n\nDoH-17072021-48h.pcapng.trapcap.csv size: 787199 ; sampled size: 78720\n\n\nDoH-19072021-48h.pcapng.trapcap.csv size: 1349750 ; sampled size: 134975\n\n\nDoH-27072021-48h.pcapng.trapcap.csv size: 1241409 ; sampled size: 124141\n\n\nDoH-28062021-24h.pcapng.trapcap.csv size: 1019033 ; sampled size: 101903\n\n\nDoH-30072021-48h.pcapng.trapcap.csv size: 633728 ; sampled size: 63373\n\n\n\nprefix = basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100/\"\nprint(\"files:\", len(list(enum_csv(prefix))))\n\nfor i, f in enumerate(f for f in enum_csv(prefix) if ('DoH-Real-World' in str(f)) and ('HTTPS' in os.path.basename(f))):\n    if i == 0:\n        kwargs = {\"mode\": \"w\", \"header\": True}\n    else:\n        kwargs = {\"mode\": \"a\", \"header\": False}\n\n    df = pd.read_csv(f)\n    df_sampled = df.sample(frac=1.0, random_state=42)  # no sampling\n    print(os.path.basename(f), \"size:\", len(df), \"; sampled size:\", len(df_sampled))\n    df_sampled.to_csv(\n        basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100-sample-1-real-world-https.csv\", \n        **kwargs\n    )\n\nfiles: 81\nHTTPS-04102021-01h-1.pcapng.trapcap.csv size: 30575 ; sampled size: 30575\n\n\nHTTPS-04102021-01h-2.pcapng.trapcap.csv size: 35537 ; sampled size: 35537\n\n\nHTTPS-04102021-02h.pcapng.trapcap.csv size: 33113 ; sampled size: 33113\n\n\nHTTPS-20102021-10h.pcapng.trapcap.csv size: 104214 ; sampled size: 104214\n\n\nHTTPS-20102021-12h.pcapng.trapcap.csv size: 85777 ; sampled size: 85777\n\n\nHTTPS-21102021-12h.pcapng.trapcap.csv size: 81300 ; sampled size: 81300\n\n\n\nunirec-csv-p100/unirec/DoH-Gen-C-CFGHOQS/data/generated/pcap/chrome/ffmuc/1_chrome_ffmuc.pcap.trapcap.csv is empty, I will remove it\nQuestion: Real-World dataset contains pcap, however readme states it can’t be distributed because requires anonymization\n\n\ndf = pd.read_csv(basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100-sample-01-real-world-doh.csv\")\ndf.to_feather(basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100-sample-01-real-world-doh.ft\")\nlen(df)\n\n1448394\n\n\n\ndf = pd.read_csv(basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100-sample-1-real-world-https.csv\")\ndf.to_feather(basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100-sample-1-real-world-https.ft\")\nlen(df)\n\n370516\n\n\n\n\n5.1.2 Sample 50k DoH and 50k HTTPS\n\n# df = pd.read_feather(\"/jupyter/warehouse/Jerabek2022Collection-unirec/unirec-csv-p100-sample-001-real-world-doh.ft\")\n# df.sample(n=50000, random_state=42).reset_index(drop=True).to_feather(\n#     \"/jupyter/warehouse/Jerabek2022Collection-unirec/unirec-csv-p100-sample-50k-real-world-doh.ft\"\n# )\n\n# df = pd.read_feather(\"/jupyter/warehouse/Jerabek2022Collection-unirec/unirec-csv-p100-sample-001-real-world-https.ft\")\n# df.sample(n=50000, random_state=42).reset_index(drop=True).to_feather(\n#     \"/jupyter/warehouse/Jerabek2022Collection-unirec/unirec-csv-p100-sample-50k-real-world-https.ft\"\n# )"
  },
  {
    "objectID": "notebooks/sample-Jerabek2022-dataset.html#generated-traffic-sampling",
    "href": "notebooks/sample-Jerabek2022-dataset.html#generated-traffic-sampling",
    "title": "5  Sample learning dataset",
    "section": "5.2 Generated traffic sampling",
    "text": "5.2 Generated traffic sampling\n\n5.2.1 Sampled to 10%:\n\nprefix = basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100/\"\nprint(\"files:\", len(list(enum_csv(prefix))))\n\nfor i, f in enumerate(f for f in enum_csv(prefix) if 'DoH-Real-World' not in str(f)):\n    if i == 0:\n        kwargs = {\"mode\": \"w\", \"header\": True}\n    else:\n        kwargs = {\"mode\": \"a\", \"header\": False}\n\n    df = pd.read_csv(f)\n    df_sampled = df.sample(frac=0.1, random_state=42)\n    print(os.path.basename(f), \"size:\", len(df), \"; sampled size:\", len(df_sampled))\n    df_sampled.to_csv(\n        basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100-sample-01-generated.csv\", \n        **kwargs\n    )\n\nfiles: 81\n0_chrome_adguard.pcap.trapcap.csv size: 23652 ; sampled size: 2365\n1_chrome_adguard.pcap.trapcap.csv size: 23001 ; sampled size: 2300\n0_chrome_ahadns.pcap.trapcap.csv size: 23618 ; sampled size: 2362\n1_chrome_ahadns.pcap.trapcap.csv size: 23063 ; sampled size: 2306\n0_chrome_blahdns.pcap.trapcap.csv size: 18648 ; sampled size: 1865\n1_chrome_blahdns.pcap.trapcap.csv size: 19073 ; sampled size: 1907\n0_chrome_bravedns.pcap.trapcap.csv size: 35271 ; sampled size: 3527\n1_chrome_bravedns.pcap.trapcap.csv size: 35753 ; sampled size: 3575\n0_chrome_comcast.pcap.trapcap.csv size: 37097 ; sampled size: 3710\n1_chrome_comcast.pcap.trapcap.csv size: 38072 ; sampled size: 3807\n0_chrome_cznic.pcap.trapcap.csv size: 34198 ; sampled size: 3420\n1_chrome_cznic.pcap.trapcap.csv size: 33623 ; sampled size: 3362\n0_chrome_cloudflare.pcap.trapcap.csv size: 30804 ; sampled size: 3080\n1_chrome_cloudflare.pcap.trapcap.csv size: 34577 ; sampled size: 3458\n0_chrome_ffmuc.pcap.trapcap.csv size: 11669 ; sampled size: 1167\n0_chrome_ffmuc.pcap.trapcap-checkpoint.csv size: 11669 ; sampled size: 1167\n0_chrome_google.pcap.trapcap.csv size: 34101 ; sampled size: 3410\n1_chrome_google.pcap.trapcap.csv size: 33109 ; sampled size: 3311\n0_chrome_hostux.pcap.trapcap.csv size: 31416 ; sampled size: 3142\n1_chrome_hostux.pcap.trapcap.csv size: 30769 ; sampled size: 3077\n0_chrome_opendns.pcap.trapcap.csv size: 30691 ; sampled size: 3069\n1_chrome_opendns.pcap.trapcap.csv size: 29655 ; sampled size: 2966\n0_chrome_quad9.pcap.trapcap.csv size: 32409 ; sampled size: 3241\n1_chrome_quad9.pcap.trapcap.csv size: 31478 ; sampled size: 3148\n0_chrome_switch.pcap.trapcap.csv size: 31600 ; sampled size: 3160\n1_chrome_switch.pcap.trapcap.csv size: 32517 ; sampled size: 3252\n0_chrome_dnsforge.pcap.trapcap.csv size: 20354 ; sampled size: 2035\n1_chrome_dnsforge.pcap.trapcap.csv size: 20153 ; sampled size: 2015\n0_chrome_dnssb.pcap.trapcap.csv size: 31713 ; sampled size: 3171\n1_chrome_dnssb.pcap.trapcap.csv size: 32421 ; sampled size: 3242\n0_chrome_dohli.pcap.trapcap.csv size: 19 ; sampled size: 2\n1_chrome_dohli.pcap.trapcap.csv size: 35955 ; sampled size: 3596\n10_post_adguard.pcap.trapcap.csv size: 164808 ; sampled size: 16481\n11_get_adguard.pcap.trapcap.csv size: 166665 ; sampled size: 16666\n14_post_ahadns.pcap.trapcap.csv size: 116815 ; sampled size: 11682\n15_get_ahadns.pcap.trapcap.csv size: 115051 ; sampled size: 11505\n6_post_blahdns.pcap.trapcap.csv size: 177242 ; sampled size: 17724\n7_get_blahdns.pcap.trapcap.csv size: 177928 ; sampled size: 17793\n16_post_bravedns.pcap.trapcap.csv size: 110221 ; sampled size: 11022\n17_get_bravedns.pcap.trapcap.csv size: 108565 ; sampled size: 10856\n12_post_cloudflare.pcap.trapcap.csv size: 104344 ; sampled size: 10434\n13_get_cloudflare.pcap.trapcap.csv size: 105946 ; sampled size: 10595\n4_post_comcast.pcap.trapcap.csv size: 148266 ; sampled size: 14827\n5_get_comcast.pcap.trapcap.csv size: 151250 ; sampled size: 15125\n20_post_cznic.pcap.trapcap.csv size: 142975 ; sampled size: 14298\n21_get_cznic.pcap.trapcap.csv size: 137961 ; sampled size: 13796\n8_post_dnsforge.pcap.trapcap.csv size: 137647 ; sampled size: 13765\n9_get_dnsforge.pcap.trapcap.csv size: 142600 ; sampled size: 14260\n2_post_dnssb.pcap.trapcap.csv size: 118262 ; sampled size: 11826\n3_get_dnssb.pcap.trapcap.csv size: 116030 ; sampled size: 11603\n24_post_dohli.pcap.trapcap.csv size: 148400 ; sampled size: 14840\n25_get_dohli.pcap.trapcap.csv size: 146729 ; sampled size: 14673\n30_post_ffmuc.pcap.trapcap.csv size: 102224 ; sampled size: 10222\n31_get_ffmuc.pcap.trapcap.csv size: 102094 ; sampled size: 10209\n18_post_google.pcap.trapcap.csv size: 140524 ; sampled size: 14052\n19_get_google.pcap.trapcap.csv size: 141396 ; sampled size: 14140\n28_post_hostux.pcap.trapcap.csv size: 125728 ; sampled size: 12573\n29_get_hostux.pcap.trapcap.csv size: 121226 ; sampled size: 12123\n22_post_opendns.pcap.trapcap.csv size: 100798 ; sampled size: 10080\n23_get_opendns.pcap.trapcap.csv size: 101153 ; sampled size: 10115\n26_post_quad9.pcap.trapcap.csv size: 135104 ; sampled size: 13510\n27_get_quad9.pcap.trapcap.csv size: 135357 ; sampled size: 13536\n0_post_switch.pcap.trapcap.csv size: 147445 ; sampled size: 14744\n1_get_switch.pcap.trapcap.csv size: 150962 ; sampled size: 15096\n\n\n\ndf = pd.read_csv(basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100-sample-01-generated.csv\")\ndf.to_feather(basepath + \"datasets/Jerabek2022Collection-unirec/unirec-csv-p100-sample-01-generated.ft\")\nlen(df)\n\n513386\n\n\n\n\n5.2.2 Sampled to 50k records\n\n# df = pd.read_feather(\"/jupyter/warehouse/Jerabek2022Collection-unirec/unirec-csv-p100-sample-005-generated.ft\")\n# df.sample(n=50000, random_state=42).reset_index(drop=True).to_feather(\n#     \"/jupyter/warehouse/Jerabek2022Collection-unirec/unirec-csv-p100-sample-50k-generated.ft\"\n# )"
  },
  {
    "objectID": "notebooks/cached-preprocessing-and-labeling.html#build-learning-dataset",
    "href": "notebooks/cached-preprocessing-and-labeling.html#build-learning-dataset",
    "title": "6  Label learning dataset",
    "section": "6.1 Build learning dataset",
    "text": "6.1 Build learning dataset\nTake 10% of generated traffic (~50k FIXME) and 0.3% (~50k FIXME) of real-world traffic to get 1:1 ratio in the learning dataset.\n\np1version = \"v6\"\n\n\ngen = pd.read_feather(\n    basepath + \"datasets/Jerabek2022Collection-unirec/\"\n    \"unirec-csv-p100-sample-01-generated.ft\"\n)\nreal_doh = pd.read_feather(\n    basepath + \"datasets/Jerabek2022Collection-unirec/\"\n    \"unirec-csv-p100-sample-01-real-world-doh.ft\"\n).sample(frac=0.5)\nreal_https = pd.read_feather(\n    basepath + \"datasets/Jerabek2022Collection-unirec/\"\n    \"unirec-csv-p100-sample-1-real-world-https.ft\"\n)\n\n\nimport matplotlib.pyplot as plt\npd.DataFrame(\n    {\n        \"Rows\": [len(gen), len(real_doh), len(real_https)],\n        \"Dataset\": [\n            f\"Generated {len(gen)}\", \n            f\"Real-world DoH {len(real_doh)}\", \n            f\"Real-world HTTPS {len(real_https)}\",\n        ]\n    }\n).set_index(\"Dataset\")[\"Rows\"].plot(kind=\"pie\")\nplt.plot()\n\n[]\n\n\n\n\n\n\npath_doh_ips = \"./doh_resolver_ip.csv\"\n\ncp = CacheableProcessing(100, 100, get_doh_ips(path_doh_ips))\nprint(\"Preprocessing of generated traffic...\")\ngen_p = cp.process(gen, f\"cached/gen.p1.{p1version}.saved\")\n\nPreprocessing of generated traffic...\n\n\n\nprint(\"Preprocessing of real-world DoH traffic...\")\nreal_doh_p = cp.process(real_doh, f\"cached/real_doh05.p1.{p1version}.saved\")\n\nPreprocessing of real-world DoH traffic...\n\n\n\nprint(\"Preprocessing of real-world HTTPS traffic...\")\nreal_https_p = cp.process(real_https, f\"cached/real_https.p1.{p1version}.saved\")\n\nPreprocessing of real-world HTTPS traffic...\n\n\n\ngen_p_sig = filter_significant_flows(gen_p)\nreal_doh_p_sig = filter_significant_flows(real_doh_p)\nreal_https_p_sig = filter_significant_flows(real_https_p)\n\n\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n\npd.DataFrame(\n    {\n        \"Rows\": [len(gen_p), len(real_doh_p), len(real_https_p)],\n        \"Dataset\": [\n            f\"Generated {len(gen_p)}\", \n            f\"Real-world DoH {len(real_doh_p)}\", \n            f\"Real-world HTTPS {len(real_https_p)}\",\n        ]\n    }\n).set_index(\"Dataset\")[\"Rows\"].plot(kind=\"pie\", ax=axs[0])\n\npd.DataFrame(\n    {\n        \"Rows\": [len(gen_p_sig), len(real_doh_p_sig), len(real_https_p_sig)],\n        \"Dataset\": [\n            f\"Generated {len(gen_p_sig)}\", \n            f\"Real-world DoH {len(real_doh_p_sig)}\", \n            f\"Real-world HTTPS {len(real_https_p_sig)}\",\n        ]\n    }\n).set_index(\"Dataset\")[\"Rows\"].plot(kind=\"pie\", ax=axs[1])\nplt.plot()\n\n[]\n\n\n\n\n\n\ngen_s = gen_p_sig.sample(n=150000, random_state=42)\nreal_doh_s = real_doh_p_sig.sample(n=125000, random_state=42)\nreal_https_s = real_https_p_sig.sample(n=75000, random_state=42)\nmix_p = pd.concat([\n    gen_s,\n    real_doh_s,\n    real_https_s,\n]).reset_index(drop=True)\nlen(mix_p)\n\n350000\n\n\n\nimport matplotlib.pyplot as plt\npd.DataFrame(\n    {\n        \"Rows\": [len(gen_s), len(real_doh_s), len(real_https_s)],\n        \"Dataset\": [\n            f\"Generated {len(gen_s)}\", \n            f\"Real-world DoH {len(real_doh_s)}\", \n            f\"Real-world HTTPS {len(real_https_s)}\",\n        ]\n    }\n).set_index(\"Dataset\")[\"Rows\"].plot(kind=\"pie\")\nplt.plot()\n\n[]\n\n\n\n\n\n\nmix_p.to_feather(f\"cached/mixed.p1.{p1version}.saved\")\nmix_p = pd.read_feather(f\"cached/mixed.p1.{p1version}.saved\")\nlen(mix_p)\n\n350000\n\n\n\nmix_p.info()\n\n&lt;class 'modin.pandas.dataframe.DataFrame'&gt;\nRangeIndex: 350000 entries, 0 to 349999\nData columns (total 36 columns):\n #   Column                      Non-Null Count   Dtype \n---  --------------------------  ---------------  ----- \n 0   Unnamed: 0                  350000 non-null  int64\n 1   ipaddr DST_IP               350000 non-null  object\n 2   ipaddr SRC_IP               350000 non-null  object\n 3   uint64 BYTES                350000 non-null  int64\n 4   uint64 BYTES_REV            350000 non-null  int64\n 5   uint64 LINK_BIT_FIELD       350000 non-null  int64\n 6   time TIME_FIRST             350000 non-null  object\n 7   time TIME_LAST              350000 non-null  object\n 8   macaddr DST_MAC             350000 non-null  object\n 9   macaddr SRC_MAC             350000 non-null  object\n 10  uint32 PACKETS              350000 non-null  int64\n 11  uint32 PACKETS_REV          350000 non-null  int64\n 12  uint16 DST_PORT             350000 non-null  int64\n 13  uint16 SRC_PORT             350000 non-null  int64\n 14  uint16 TLS_VERSION          350000 non-null  int64\n 15  uint8 DIR_BIT_FIELD         350000 non-null  int64\n 16  uint8 PROTOCOL              350000 non-null  int64\n 17  uint8 TCP_FLAGS             350000 non-null  int64\n 18  uint8 TCP_FLAGS_REV         350000 non-null  int64\n 19  int8* PPI_PKT_DIRECTIONS    350000 non-null  object\n 20  uint8* PPI_PKT_FLAGS        350000 non-null  object\n 21  string TLS_ALPN             21319 non-null   object\n 22  bytes TLS_JA3               138455 non-null  object\n 23  string TLS_SNI              138450 non-null  object\n 24  uint16* PPI_PKT_LENGTHS     350000 non-null  object\n 25  uint32* D_PHISTS_IPT        350000 non-null  object\n 26  uint32* D_PHISTS_SIZES      350000 non-null  object\n 27  uint32* S_PHISTS_IPT        350000 non-null  object\n 28  uint32* S_PHISTS_SIZES      350000 non-null  object\n 29  time* PPI_PKT_TIMES         350000 non-null  object\n 30  uint16* PPI_PKT_LENGTHS_1   350000 non-null  object\n 31  uint16* PPI_PKT_LENGTHS_-1  350000 non-null  object\n 32  PPI_PKT_INTERVALS           350000 non-null  object\n 33  PPI_PKT_INTERVALS_1         350000 non-null  object\n 34  PPI_PKT_INTERVALS-1         350000 non-null  object\n 35  IsDoH                       350000 non-null  bool \ndtypes: object(22), int64(13), bool(1)\nmemory usage: 93.8 MB\n\n\n\nmix_p[\"IsDoH\"].value_counts().plot(kind=\"pie\")\nplt.plot()\n\n[]\n\n\n\n\n\n\n\n\n\nJeřábek, Kamil, Karel Hynek, Tomáš Čejka, and Ondřej Ryšavý. 2022. “Collection of Datasets with DNS over HTTPS Traffic.” Data in Brief 42 (June): 108310. https://doi.org/10.1016/j.dib.2022.108310."
  },
  {
    "objectID": "notebooks/build-models.html#sec-build-models",
    "href": "notebooks/build-models.html#sec-build-models",
    "title": "7  Build models",
    "section": "7.1 Build models with different hyperparameters",
    "text": "7.1 Build models with different hyperparameters\nModels to evaluate:\n\nLogisticRegression (only for the baseline)\nRandomForest\nHistGradientBoosting\n\nParameters:\n\nNormalization of statistical parameters (hypothesis: should decrease concept drift)\nSkip of first few packets to decrease TLS handshake influence on classification of DoH\n\nTry to apply weights to first few packets for calculation of mean/stdev/variance, instead of just skip\nTry to skip packets for packet sizes parameters, but don’t skip them for inter-packets duration parameters\n\n\n\np1version = \"v6\"\np2version = \"v7\"\n\n\npath_doh_ips = \"./doh_resolver_ip.csv\"\n\nmix_p = pd.read_feather(f\"cached/mixed.p1.{p1version}.saved\")\n\n\nlen(mix_p)\n\n350000\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(\n    mix_p,\n    test_size=0.2,\n    random_state=42\n)\n\ntrain.reset_index(drop=True).to_feather(f'cached/mixed.train.p1.{p1version}.saved')\ntest.reset_index(drop=True).to_feather(f'cached/mixed.test.p1.{p1version}.saved')\n\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom joblib import dump, load\n\ntrain = pd.read_feather(f'cached/mixed.train.p1.{p1version}.saved')\n\nfor norm in [True, False]:\n    for hyperparam in HYPERPARAMS:\n        pp = PreProcessing(norm, *hyperparam)\n        name = (\n            ('normed-' if norm else 'unnormed-') + \n            '-'.join(str(p) for p in hyperparam)\n        )\n\n        print(name)\n        clr_lr_file = f'models/LR-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib'\n        clf_hgb_file = f'models/HGB-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib'\n        clf_rf_file = f'models/RF-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib'\n\n        if any(not os.path.exists(fpath) for fpath in [clr_lr_file, clf_hgb_file, clf_rf_file]):\n            full, features, labels = pp.process(train, f\"cached/mixed.train.p1.{p1version}.{name}.p2.{p2version}.saved\")\n            pp.store_scaler(f\"cached/mixed.train.p1.{p1version}.{name}.p2.{p2version}.scaler\")\n\n            if not os.path.exists(clr_lr_file):\n                clf_lr = LogisticRegression(random_state=42, max_iter=200).fit(features, labels)\n                dump(clf_lr, clr_lr_file)\n\n            if not os.path.exists(clf_hgb_file):\n                clf_hgb = HistGradientBoostingClassifier(random_state=42, max_iter=200).fit(features, labels)\n                dump(clf_hgb, clf_hgb_file)\n\n            if not os.path.exists(clf_rf_file):\n                clf_rf = RandomForestClassifier(random_state=42).fit(features, labels)\n                dump(clf_rf, clf_rf_file)\n\nnormed-0-0-0-0\nnormed-0-4-0-0\nnormed-1-0-0-0\nnormed-2-0-0-0\nnormed-4-0-0-0\nnormed-6-0-0-0\nnormed-1-4-0-0\nnormed-2-0-0-0\nnormed-2-4-0-0\nnormed-0-4-0-4\nnormed-1-0-1-0\nnormed-2-0-2-0\nnormed-4-0-4-0\nnormed-6-0-6-0\nnormed-1-4-1-4\nnormed-2-0-2-0\nnormed-2-4-2-4\nunnormed-0-0-0-0\nunnormed-0-4-0-0\nunnormed-1-0-0-0\nunnormed-2-0-0-0\nunnormed-4-0-0-0\nunnormed-6-0-0-0\nunnormed-1-4-0-0\nunnormed-2-0-0-0\nunnormed-2-4-0-0\nunnormed-0-4-0-4\nunnormed-1-0-1-0\nunnormed-2-0-2-0\nunnormed-4-0-4-0\nunnormed-6-0-6-0\nunnormed-1-4-1-4\nunnormed-2-0-2-0\nunnormed-2-4-2-4"
  },
  {
    "objectID": "notebooks/build-models.html#calculate-auc-for-each-model",
    "href": "notebooks/build-models.html#calculate-auc-for-each-model",
    "title": "7  Build models",
    "section": "7.2 Calculate AUC for each model",
    "text": "7.2 Calculate AUC for each model\n\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom joblib import dump, load\n\ntest = pd.read_feather(f'cached/mixed.test.p1.{p1version}.saved')\n\nmodels = [\"LR\", \"HGB\", \"RF\"]\n\nresults = {\n    \"model\": [],\n    \"normed\": [],\n    \"skips_and_weights\": [],\n    \"test_data_auc\": [],\n    \"fpr\": [],\n    \"tpr\": [],\n    \"thresholds\": [],\n}\n\nfor norm in [True, False]:\n    for hyperparam in HYPERPARAMS:\n        pp = PreProcessing(norm, *hyperparam)\n        name = (\n            ('normed-' if norm else 'unnormed-') + \n            '-'.join(str(p) for p in hyperparam)\n        )\n\n        print(name)\n\n        full, features, labels = pp.process(test, f\"cached/mixed.test.p1.{p1version}.{name}.p2.{p2version}.saved\")\n\n        for model in models:\n            clf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n            y_pred = clf.predict_proba(features)[:, 1]\n            fpr, tpr, threshold = metrics.roc_curve(labels, y_pred)\n            auc = round(metrics.roc_auc_score(labels, y_pred), 4)\n            \n            results[\"model\"].append(model)\n            results[\"normed\"].append(norm)\n            results[\"skips_and_weights\"].append(hyperparam)\n            results[\"test_data_auc\"].append(auc)\n            results[\"fpr\"].append(fpr)\n            results[\"tpr\"].append(tpr)\n            results[\"thresholds\"].append(threshold)\n\nnormed-0-0-0-0\n\n\nnormed-0-4-0-0\n\n\nnormed-1-0-0-0\n\n\nnormed-2-0-0-0\n\n\nnormed-4-0-0-0\n\n\nnormed-6-0-0-0\n\n\nnormed-1-4-0-0\n\n\nnormed-2-0-0-0\n\n\nnormed-2-4-0-0\n\n\nnormed-0-4-0-4\n\n\nnormed-1-0-1-0\n\n\nnormed-2-0-2-0\n\n\nnormed-4-0-4-0\n\n\nnormed-6-0-6-0\n\n\nnormed-1-4-1-4\n\n\nnormed-2-0-2-0\n\n\nnormed-2-4-2-4\n\n\nunnormed-0-0-0-0\n\n\nunnormed-0-4-0-0\n\n\nunnormed-1-0-0-0\n\n\nunnormed-2-0-0-0\n\n\nunnormed-4-0-0-0\n\n\nunnormed-6-0-0-0\n\n\nunnormed-1-4-0-0\n\n\nunnormed-2-0-0-0\n\n\nunnormed-2-4-0-0\n\n\nunnormed-0-4-0-4\n\n\nunnormed-1-0-1-0\n\n\nunnormed-2-0-2-0\n\n\nunnormed-4-0-4-0\n\n\nunnormed-6-0-6-0\n\n\nunnormed-1-4-1-4\n\n\nunnormed-2-0-2-0\n\n\nunnormed-2-4-2-4\n\n\n\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\n\n\n\n\n\nmodel\nnormed\nskips_and_weights\ntest_data_auc\nfpr\ntpr\nthresholds\n\n\n\n\n0\nLR\nTrue\n[0, 0, 0, 0]\n0.9377\n[0.0, 0.0, 0.0, 2.440393391414696e-05, 2.44039...\n[0.0, 3.445543189883885e-05, 0.000964752093167...\n[1.9999996864974432, 0.9999996864974431, 0.999...\n\n\n1\nHGB\nTrue\n[0, 0, 0, 0]\n0.9957\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n[0.0, 3.445543189883885e-05, 0.000275643455190...\n[1.9998165372259558, 0.9998165372259558, 0.999...\n\n\n2\nRF\nTrue\n[0, 0, 0, 0]\n0.9970\n[0.0, 2.440393391414696e-05, 4.880786782829392...\n[0.0, 0.07525066326706405, 0.14460944767942666...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n3\nLR\nTrue\n[0, 4, 0, 0]\n0.9380\n[0.0, 0.0, 0.0, 2.440393391414696e-05, 2.44039...\n[0.0, 3.445543189883885e-05, 0.001619405299245...\n[1.999995182014497, 0.9999951820144969, 0.9992...\n\n\n4\nHGB\nTrue\n[0, 4, 0, 0]\n0.9956\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n[0.0, 3.445543189883885e-05, 0.000241188023291...\n[1.9998026699943354, 0.9998026699943353, 0.999...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n97\nHGB\nFalse\n[2, 0, 2, 0]\n0.9977\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n[0.0, 6.89108637976777e-05, 0.0001722771594941...\n[1.9994627828151588, 0.9994627828151587, 0.999...\n\n\n98\nRF\nFalse\n[2, 0, 2, 0]\n0.9991\n[0.0, 0.0, 4.880786782829392e-05, 7.3211801742...\n[0.0, 0.660028253454157, 0.7572270268407815, 0...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n99\nLR\nFalse\n[2, 4, 2, 4]\n0.8475\n[0.0, 0.0, 0.0, 0.0, 2.440393391414696e-05, 2....\n[0.0, 0.0008613857974709713, 0.000930296661268...\n[2.0, 1.0, 0.9999999999999998, 0.9998291078664...\n\n\n100\nHGB\nFalse\n[2, 4, 2, 4]\n0.9975\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n[0.0, 3.445543189883885e-05, 0.000379009750887...\n[1.9995824205222212, 0.9995824205222211, 0.999...\n\n\n101\nRF\nFalse\n[2, 4, 2, 4]\n0.9989\n[0.0, 0.0, 0.0, 2.440393391414696e-05, 7.32118...\n[0.0, 0.644867863418668, 0.7421355476690901, 0...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n\n\n102 rows x 7 columns\n\n\n\n\nresults_df.to_feather(f\"cached/p1.{p1version}.p2.{p2version}.ft\")\n\n\nresults_df.sort_values(\"test_data_auc\", ascending=False)\n\n\n\n\n\n\n\n\nmodel\nnormed\nskips_and_weights\ntest_data_auc\nfpr\ntpr\nthresholds\n\n\n\n\n83\nRF\nFalse\n[1, 0, 1, 0]\n0.9994\n[0.0, 2.440393391414696e-05, 2.440393391414696...\n[0.0, 0.66943458636254, 0.7606036591668677, 0....\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n53\nRF\nFalse\n[0, 0, 0, 0]\n0.9993\n[0.0, 4.880786782829392e-05, 7.321180174244088...\n[0.0, 0.6811494332081452, 0.7709402887365193, ...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n59\nRF\nFalse\n[1, 0, 0, 0]\n0.9993\n[0.0, 7.321180174244088e-05, 0.000146423603484...\n[0.0, 0.6717086448678634, 0.7633256382868759, ...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n80\nRF\nFalse\n[0, 4, 0, 4]\n0.9992\n[0.0, 4.880786782829392e-05, 7.321180174244088...\n[0.0, 0.6669882506977225, 0.7550218791992558, ...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n56\nRF\nFalse\n[0, 4, 0, 0]\n0.9992\n[0.0, 4.880786782829392e-05, 9.761573565658785...\n[0.0, 0.6723977535058402, 0.7586741549805327, ...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n63\nLR\nFalse\n[4, 0, 0, 0]\n0.7864\n[0.0, 0.0, 0.0, 0.0, 0.0, 2.440393391414696e-0...\n[0.0, 0.0001378217275953554, 0.000206732591393...\n[2.0, 1.0, 0.9999999999999805, 0.9999999999999...\n\n\n66\nLR\nFalse\n[6, 0, 0, 0]\n0.7800\n[0.0, 0.0, 0.0, 2.440393391414696e-05, 2.44039...\n[0.0, 3.445543189883885e-05, 0.001412672707852...\n[2.0, 1.0, 0.9977177679966023, 0.9976716560895...\n\n\n36\nLR\nTrue\n[4, 0, 4, 0]\n0.7732\n[0.0, 0.0, 0.0, 9.761573565658785e-05, 9.76157...\n[0.0, 3.445543189883885e-05, 0.000172277159494...\n[2.0, 1.0, 0.9999988537637468, 0.9999979947780...\n\n\n90\nLR\nFalse\n[6, 0, 6, 0]\n0.7346\n[0.0, 0.0, 0.0, 7.321180174244088e-05, 7.32118...\n[0.0, 3.445543189883885e-05, 0.000792474933673...\n[1.9999999999998852, 0.9999999999998852, 0.985...\n\n\n39\nLR\nTrue\n[6, 0, 6, 0]\n0.6395\n[0.0, 0.0, 0.0, 4.880786782829392e-05, 4.88078...\n[0.0, 0.00024118802329187197, 0.00037900975088...\n[2.0, 1.0, 0.9999999927725642, 0.9999999777275...\n\n\n\n\n102 rows x 7 columns\n\n\n\n\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\nresults_df = pd.read_feather(f\"cached/p1.{p1version}.p2.{p2version}.ft\")\n\n# plt.figure(0).clf()\n\nfor model in models:\n    fig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n    for i, norm in enumerate([True, False]):\n        for index, row in results_df[\n            (results_df[\"model\"] == model) & (results_df[\"normed\"] == norm)\n        ].iterrows():\n            axs[i].plot(\n                row[\"fpr\"],\n                row[\"tpr\"],\n                label=f\"{row['model']} {'normed' if row['normed'] else 'unnormed'} {','.join([str(i) for i in row['skips_and_weights']])} AUC: {row['test_data_auc']}\",\n            )\n\n        axs[i].legend()\n        axs[i].set_title(f\"{model} {'normed' if norm else 'unnormed'}\")\n    plt.show()"
  },
  {
    "objectID": "notebooks/choose-model.html#sec-robustness",
    "href": "notebooks/choose-model.html#sec-robustness",
    "title": "8  Choose model",
    "section": "8.1 Evaluate models on concept drift robustness",
    "text": "8.1 Evaluate models on concept drift robustness\nNow we have generated models with different hyperparameters:\n\nnormalization (on/off)\nskip packets for inter-packet duration statistics\nskip packets for packet sizes statistics\napply weight on inter-packet duration statistics\napply weight on packet sizes statistics\n\nMerge test set (20% split from the training set) with the benign traffic from the training part (B1 stored in v5_1.csv) generated on Android devices\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\ntest_df_p = pd.read_feather(f'cached/mixed.test.p1.{p1version}.saved')\n\n\ndf = pd.read_feather(\"cached/v5_1.ft\")\nnon_doh = df[df[\"ipaddr SRC_IP\"] == \"192.168.2.249\"]\nsome_doh = df[df[\"ipaddr SRC_IP\"] == \"192.168.2.149\"]\n\n\npath_doh_ips_androids = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_general = \"doh_resolver_ip.csv\"\nfull_list = get_doh_ips(path_doh_ips_androids) + get_doh_ips(path_doh_ips_general)\n\ncp = CacheableProcessing(100, 100, full_list)\nnon_doh_p = cp.process(non_doh, f\"cached/non_doh.p3.{p3version}.saved\")\nsome_doh_p = cp.process(some_doh, f\"cached/some_doh.p3.{p3version}.saved\")\n\n\nlen(some_doh_p[some_doh_p[\"IsDoH\"] == True])\n\n155\n\n\n\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n\nsome_doh_p[\"IsDoH\"].value_counts().plot(kind=\"pie\", ax=axs[0])\nnon_doh_p[\"IsDoH\"].value_counts().plot(kind=\"pie\", ax=axs[1])\nplt.plot()\n\n[]\n\n\n\n\n\n\nandroid_joined = pd.concat([some_doh_p, non_doh_p])\nlen(android_joined)\n\n17598\n\n\n\ntest_df_p[\"IsDoH\"].value_counts()\n\nFalse    40977\nTrue     29023\nName: IsDoH, dtype: int64\n\n\n\nandroid_joined[\"IsDoH\"].value_counts()\n\nFalse    17443\nTrue       155\nName: IsDoH, dtype: int64\n\n\nBalance dataset by oversampling the DoH class\n\nandroid_doh = android_joined[android_joined[\"IsDoH\"] == True].sample(n=5000, replace=True, random_state=42)\nandroid_non_doh = android_joined[android_joined[\"IsDoH\"] == False].sample(n=5000, replace=True, random_state=42)\n\ntest_df_doh = test_df_p[test_df_p[\"IsDoH\"] == True].sample(n=5000, replace=True, random_state=42)\ntest_df_non_doh = test_df_p[test_df_p[\"IsDoH\"] == False].sample(n=5000, replace=True, random_state=42)\n\nMix test dataset and benign data collected from Android\n\nbalanced_mix = pd.concat([\n    android_doh, android_non_doh, test_df_doh, test_df_non_doh\n]).reset_index(drop=True)\nbalanced_mix = balanced_mix\nlen(balanced_mix)\n\n20000\n\n\n\nbalanced_mix[\"IsDoH\"].value_counts().plot(kind=\"pie\")\nplt.plot()\n\n[]\n\n\n\n\n\n\nbalanced_mix.reset_index(drop=True).to_feather(f'cached/robustness.balanced_mix.p3.{p3version}.ft')\n\nNow we have a dataset which contains from\n\n25% of DoH traffic from the training environment\n25% of non-DoH traffic from the training environment\n25% of DoH traffic from the production environment\n25% of non-DoH traffic from the production environment\n\n\nfrom joblib import dump, load\nfrom sklearn import metrics\n\ntest = pd.read_feather(f'cached/robustness.balanced_mix.p3.{p3version}.ft')\n\nmodels = [\"LR\", \"HGB\", \"RF\"]\n\nresults = {\n    \"model\": [],\n    \"normed\": [],\n    \"skips_and_weights\": [],\n    \"test_data_auc\": [],\n    \"fpr\": [],\n    \"tpr\": [],\n    \"thresholds\": [],\n}\n\nfor norm in [True, False]:\n    for hyperparam in HYPERPARAMS:\n        pp = PreProcessing(norm, *hyperparam)\n        name = (\n            ('normed-' if norm else 'unnormed-') + \n            '-'.join(str(p) for p in hyperparam)\n        )\n\n        print(name)\n\n        full, features, labels = pp.process(test, f\"cached/robustness.balanced_mix.p3.{p3version}.{name}.saved\")\n\n        for model in models:\n            clf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n            y_pred = clf.predict_proba(features)[:, 1]\n            fpr, tpr, threshold = metrics.roc_curve(labels, y_pred)\n            auc = round(metrics.roc_auc_score(labels, y_pred), 4)\n\n            results[\"model\"].append(model)\n            results[\"normed\"].append(norm)\n            results[\"skips_and_weights\"].append(hyperparam)\n            results[\"test_data_auc\"].append(auc)\n            results[\"fpr\"].append(fpr)\n            results[\"tpr\"].append(tpr)\n            results[\"thresholds\"].append(threshold)\n\nnormed-0-0-0-0\n\n\nnormed-1-0-0-0\n\n\nnormed-2-0-0-0\n\n\nnormed-4-0-0-0\n\n\nnormed-6-0-0-0\n\n\nnormed-0-4-0-0\n\n\nnormed-1-4-0-0\n\n\nnormed-2-4-0-0\n\n\nnormed-1-0-1-0\n\n\nnormed-2-0-2-0\n\n\nnormed-4-0-4-0\n\n\nnormed-6-0-6-0\n\n\n\nresults_df = pd.DataFrame(results)\nresults_df.sort_values(\"test_data_auc\", ascending=False)\n\n\n\n\n\n\n\n\nmodel\nnormed\nskips_and_weights\ntest_data_auc\nfpr\ntpr\nthresholds\n\n\n\n\n53\nRF\nFalse\n[2, 0, 0, 0]\n0.9780\n[0.0, 0.0, 0.0, 0.0001079447322970639, 0.00010...\n[0.0, 0.3485738255033557, 0.3976510067114094, ...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n68\nRF\nFalse\n[2, 4, 0, 0]\n0.9761\n[0.0, 0.0, 0.0, 0.0001079447322970639, 0.00021...\n[0.0, 0.34133808724832215, 0.3921979865771812,...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n86\nRF\nFalse\n[1, 4, 1, 4]\n0.9753\n[0.0, 0.0, 0.0, 0.0002158894645941278, 0.00021...\n[0.0, 0.3493078859060403, 0.39890939597315433,...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n74\nRF\nFalse\n[2, 0, 2, 0]\n0.9750\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0001079447322...\n[0.0, 0.3475251677852349, 0.40058724832214765,...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n65\nRF\nFalse\n[1, 4, 0, 0]\n0.9748\n[0.0, 0.0, 0.0002158894645941278, 0.0004317789...\n[0.0, 0.35245385906040266, 0.4047818791946309,...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9\nLR\nTrue\n[4, 0, 0, 0]\n0.7774\n[0.0, 0.0, 0.0, 0.0, 0.0002158894645941278, 0....\n[0.0, 0.00020973154362416107, 0.00062919463087...\n[1.9999999999999998, 0.9999999999999998, 0.999...\n\n\n30\nLR\nTrue\n[4, 0, 4, 0]\n0.7632\n[0.0, 0.0003238341968911917, 0.000431778929188...\n[0.0, 0.00041946308724832214, 0.00041946308724...\n[2.0, 1.0, 0.9999999999928075, 0.9999999995543...\n\n\n57\nLR\nFalse\n[6, 0, 0, 0]\n0.7533\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.000107944732297063...\n[0.0, 0.00010486577181208053, 0.00041946308724...\n[2.0, 1.0, 0.9993612180547663, 0.9989779662126...\n\n\n78\nLR\nFalse\n[6, 0, 6, 0]\n0.7397\n[0.0, 0.0, 0.0, 0.0, 0.0002158894645941278, 0....\n[0.0, 0.00010486577181208053, 0.00031459731543...\n[1.999999999996223, 0.999999999996223, 0.99969...\n\n\n33\nLR\nTrue\n[6, 0, 6, 0]\n0.6265\n[0.0, 0.0, 0.0001079447322970639, 0.0001079447...\n[0.0, 0.00010486577181208053, 0.00010486577181...\n[1.5903804330127818, 0.5903804330127819, 0.513...\n\n\n\n\n90 rows x 7 columns\n\n\n\n\nresults_df.to_feather(f'cached/robustness.balanced_mix-results.p3.{p3version}.ft')\n\n\n\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version}.ft\")\n\nfor model in models:\n    fig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n    for i, norm in enumerate([True, False]):\n        for index, row in results_df[\n            (results_df[\"model\"] == model) & (results_df[\"normed\"] == norm)\n        ].iterrows():\n            axs[i].plot(\n                row[\"fpr\"],\n                row[\"tpr\"],\n                label=f\"{row['model']} {'normed' if row['normed'] else 'unnormed'} {','.join([str(i) for i in row['skips_and_weights']])} AUC: {row['test_data_auc']}\",\n            )\n\n        axs[i].legend()\n        axs[i].set_title(f\"{model} {'normed' if norm else 'unnormed'}\")\n    plt.show()\n\n\n\n\n(a) Logistic Regression\n\n\n\n\n\n\n\n(b) Hist Gradient Boosting\n\n\n\n\n\n\n\n(c) Random Forest\n\n\n\nFigure 8.1: ROC and their AUC for test dataset\n\n\n\nbest_normed_model = results_df[results_df[\"normed\"] == True].sort_values(\"test_data_auc\", ascending=False).iloc[:3]\nbest_unnormed_model = results_df[results_df[\"normed\"] == False].sort_values(\"test_data_auc\", ascending=False).iloc[:3]"
  },
  {
    "objectID": "notebooks/choose-model.html#results",
    "href": "notebooks/choose-model.html#results",
    "title": "8  Choose model",
    "section": "8.2 Results",
    "text": "8.2 Results\nTop 3 normalized and top 3 non-normalized models ranged by the AUC:\n\nbest_normed_model\n\n\n\n\n\n\n\n\nmodel\nnormed\nskips_and_weights\ntest_data_auc\nfpr\ntpr\nthresholds\n\n\n\n\n23\nRF\nTrue\n[2, 4, 0, 0]\n0.9355\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00010794...\n[0.0, 0.00010486577181208053, 0.00031459731543...\n[1.98, 0.98, 0.97, 0.96, 0.95, 0.94, 0.92, 0.9...\n\n\n44\nRF\nTrue\n[2, 4, 2, 4]\n0.9324\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.000107944732297063...\n[0.0, 0.00041946308724832214, 0.00094379194630...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n5\nRF\nTrue\n[1, 0, 0, 0]\n0.9278\n[0.0, 0.0, 0.0, 0.0, 0.0001079447322970639, 0....\n[0.0, 0.00010486577181208053, 0.00020973154362...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.95, 0.93, 0.92,...\n\n\n\n\n\n\n\n\nbest_unnormed_model\n\n\n\n\n\n\n\n\nmodel\nnormed\nskips_and_weights\ntest_data_auc\nfpr\ntpr\nthresholds\n\n\n\n\n53\nRF\nFalse\n[2, 0, 0, 0]\n0.9780\n[0.0, 0.0, 0.0, 0.0001079447322970639, 0.00010...\n[0.0, 0.3485738255033557, 0.3976510067114094, ...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n68\nRF\nFalse\n[2, 4, 0, 0]\n0.9761\n[0.0, 0.0, 0.0, 0.0001079447322970639, 0.00021...\n[0.0, 0.34133808724832215, 0.3921979865771812,...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n86\nRF\nFalse\n[1, 4, 1, 4]\n0.9753\n[0.0, 0.0, 0.0, 0.0002158894645941278, 0.00021...\n[0.0, 0.3493078859060403, 0.39890939597315433,...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n\n\n\n\n\nInterestingly, in this comparison normalization makes model perform worse. However, it doesn’t matches the original hypothesis. Let’s take both normed and unnormed models and try to use them on the production data."
  },
  {
    "objectID": "notebooks/choose-threshold.html",
    "href": "notebooks/choose-threshold.html",
    "title": "9  Selecting threshold",
    "section": "",
    "text": "In this notebook we\n\ninspect the AUC curve of top models\nselect the most interesting thresholds\ncalculate the confusion matricies for them\n\n\nfrom detect_common import *\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\npath_doh_ips_all = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_Jerabek2022 = \"./doh_resolver_ip.csv\"\n\n\ntest = pd.read_feather(f'cached/robustness.balanced_mix.p3.{p3version}.ft')\n\n\n# results_df = pd.read_feather(f\"cached/robustness-flubot-mix.p3.{p3version}.ft\")\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version}.ft\")\nresults_df[\"skips_and_weights\"] = results_df[\"skips_and_weights\"].apply(str)\n\n\nbest_normed_models = results_df[results_df[\"normed\"] == True].sort_values(\"test_data_auc\", ascending=False).iloc[:3]\nbest_unnormed_models = results_df[results_df[\"normed\"] == False].sort_values(\"test_data_auc\", ascending=False).iloc[:3]\n\n\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom joblib import dump, load\n\n\nsensitivities = [0.4, 0.6, 0.8, 0.9]\ncheck_thresholds = [0.69, 0.5, 0.18]\n\n\nfor m in [best_normed_models.iloc[0], best_unnormed_models.iloc[0]]:\n\n    model, normed, skips_and_weights, auc, fpr, tpr, thresholds = m\n    skips_and_weights = [int(i) for i in skips_and_weights.strip(\"[]\").split()]\n    name = (\n        ('normed-' if normed else 'unnormed-') + \n        '-'.join(str(p) for p in skips_and_weights)\n    )\n    clf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n    pp = PreProcessing(normed, *skips_and_weights)\n    full, features, labels = pp.process(test, f\"cached/robustness.balanced_mix.p3.{p3version}.{name}.saved\")\n\n    plt.plot(fpr,tpr,label=f\"{name} AUC={auc}\", color='red')\n\n    plt.legend()\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.show()\n\n    fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(12, 8))\n    axs = np.ravel(axs)\n\n    for i, sensitivity in enumerate(sensitivities):\n        thres = [thres1 for sens1, thres1 in zip(tpr, thresholds) if sens1 &gt;= sensitivity][0]\n\n        print(\"sensitivity:\", sensitivity, \"threshold:\", thres)\n        y_pred = (clf.predict_proba(features[pp.feature_fields])[:,1] &gt;= thres).astype(bool)\n        cm = confusion_matrix(labels, y_pred)\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['NonDoH', 'DoH'])\n        disp.plot(ax=axs[i], colorbar=False)\n        f1 = metrics.f1_score(labels, y_pred)\n        axs[i].set_title(f\"thres={thres} tpr={sensitivity} ({model} {name})\\nf1={f1:.2f}\")\n\n    fig.tight_layout()\n    display(pd.DataFrame({\"tpr\": tpr[::15], \"fpr\": fpr[::15], \"threshold\": thresholds[::15]}).T)\n    plt.show()\n\n\n\n\nsensitivity: 0.4 threshold: 0.55\n\n\nsensitivity: 0.6 threshold: 0.44\nsensitivity: 0.8 threshold: 0.34\n\n\nsensitivity: 0.9 threshold: 0.28\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\ntpr\n0.00\n0.035340\n0.186242\n0.438444\n0.734794\n0.936871\n0.995386\n\n\nfpr\n0.00\n0.000324\n0.002699\n0.013277\n0.086032\n0.242012\n0.497085\n\n\nthreshold\n1.98\n0.830000\n0.680000\n0.530000\n0.380000\n0.230000\n0.080000\n\n\n\n\n\n\n\n\n\n\n\n\n\nsensitivity: 0.4 threshold: 0.98\n\n\nsensitivity: 0.6 threshold: 0.83\n\n\nsensitivity: 0.8 threshold: 0.41\n\n\nsensitivity: 0.9 threshold: 0.28\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\ntpr\n0.0\n0.578754\n0.669673\n0.761221\n0.802223\n0.907718\n0.986577\n\n\nfpr\n0.0\n0.002483\n0.008744\n0.021373\n0.047927\n0.090350\n0.188472\n\n\nthreshold\n2.0\n0.860000\n0.710000\n0.560000\n0.410000\n0.260000\n0.110000"
  },
  {
    "objectID": "flubot-detection.html",
    "href": "flubot-detection.html",
    "title": "Experiment 1: Malware detection",
    "section": "",
    "text": "In this part we run the FluBot detection for Top 3 normed and Top 3 unnormed models, trying 4 model sensitivities in each run.\nIn this experiment we have 3 Android x86 machines:\n\n192.168.2.217 - machine without benign traffic, infected with FluBot\n192.168.2.149 - clean machine with DoH in Chrome enabled\n192.168.2.249 - control machine with plaintext DNS configured"
  },
  {
    "objectID": "flubot-detection-overview.html#sec-scaler",
    "href": "flubot-detection-overview.html#sec-scaler",
    "title": "10  Malware detection overview",
    "section": "10.1 Concept drift: Train the scaler on production data",
    "text": "10.1 Concept drift: Train the scaler on production data\nThe network environment used to collect the training dataset and the one in which model will run in production will most likely be different: collection of a large labeled dataset require special configuration and usually done in a sandbox network separate from the real world traffic. This difference includes interferes with inter-packet duration measurements, packet routes and other features that we call generally “concept drift”.\nIn this experiment we train (fit) the scaler on the training data and on test data separately, it means that the scaling ranges and percentiles used to scale the production data are differ from the ones used in the training dataset. The hypothesis is that separate scaling training may help to reduce the concept drift introduced by a different network."
  },
  {
    "objectID": "flubot-detection-overview.html#model-deployment",
    "href": "flubot-detection-overview.html#model-deployment",
    "title": "10  Malware detection overview",
    "section": "10.2 Model deployment",
    "text": "10.2 Model deployment\nOn this stage we already have the model, or rather a group of models (3 unnormed and 3 normed) which perform the best on the test data. Before we can deploy the model to the production to perform DoH detection, we should train the scaler on the new network. For that, we collect the traffic for the period of time (B1) on the timeline, without doing any inference Figure 3.1, and train the scaler on this data. In the second period (B2) we can start the malware detection module with model and the trained scaler (see Figure 10.1).\n\n\n\nFigure 10.1: Model deployment"
  },
  {
    "objectID": "flubot-detection-overview.html#doh-exfiltration-detection-algorithm",
    "href": "flubot-detection-overview.html#doh-exfiltration-detection-algorithm",
    "title": "10  Malware detection overview",
    "section": "10.3 DoH exfiltration detection algorithm",
    "text": "10.3 DoH exfiltration detection algorithm\nTo detect the malicious behavior of the machine, we propose to use the ratio of DNS requests and contacted IP addresses for every host in the local network described in the Grill et al. (2015):\n\\[\\rho(a)={\\delta(a) \\over \\pi(a) + 1}\\]\n\n\\(a\\) is the host in the network\n\\(\\delta(a)\\) values - number of DNS requests created by the host a\n\\(\\pi(a)\\) values - the number of unique IP addresses contacted by the a\n\nThe value of the ratio \\(\\rho(a)\\) represents the “specific gravity” of contacted IP addresses per DNS request and can be used to detect DGA-, fast-fluxing-based malware, DNS Tunneling, and exfiltration attacks.\nThe overall number of DNS requests \\(\\delta(a)\\) is linearly dependent on the observed duration, while the rate of increase of the count of new IPs in the time window is logarithmically decreases over time. To make the detection algorithm more robust to the change of the observed time window, we use \\(log(\\rho)\\) in our experiments.\n\n\n\n\nGrill, Martin, Ivan Nikolaev, Veronica Valeros, and Martin Rehak. 2015. “Detecting DGA Malware Using NetFlow.” In 2015 IFIP/IEEE International Symposium on Integrated Network Management (IM), 1304–9. https://doi.org/10.1109/INM.2015.7140486."
  },
  {
    "objectID": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-normed-0-0-0-0.html",
    "href": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-normed-0-0-0-0.html",
    "title": "11  Detect malware (RF normed-0-0-0-0)",
    "section": "",
    "text": "The model deployment is done in two stages: I. tune the model, II. turn on the malware detection (on the data collected in B1 and B2 periods respectively, described in the timeline Figure 3.1)\n\n\n\nTimeline\n\n\n\n11.0.1 Stage I: Model tuning\nOn this state we fit the scaler on the data collected in B1 period, to be used later on the second stage\n\nfrom detect_common import *\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\nmodel = 'RF'\nnormed = True\nskips_and_weights = [2,4,0,0]\n\n\n# Parameters\nmodel = \"RF\"\nnormed = True\nskips_and_weights = [0, 0, 0, 0]\n\n\nimport dateutil.parser\n\ndf_b1 = pd.read_feather(\"cached/v5_1.ft\")\n\n\npath_doh_ips_androids = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_general = \"doh_resolver_ip.csv\"\nfull_list = get_doh_ips(path_doh_ips_androids) + get_doh_ips(path_doh_ips_general)\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b1_p = cp.process(df_b1, f\"cached/v5_1.p3.{p3version}.cp.ft\")\n\n\nfrom joblib import dump, load\n\nname = (\n    ('normed-' if normed else 'unnormed-') + \n    '-'.join(str(p) for p in skips_and_weights)\n)\n\nclf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n\npp_b1 = PreProcessing(normed, *skips_and_weights)\n\ndf_b1_pp = pp_b1.process(\n    df_b1_p,\n    f\"cached/p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=True\n)\n\n\npp_b1.store_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\n\n\n\n11.0.2 Stage II: Malware detection\nOn this state we turn on the malware detection, provisioning the model with the scaler trained before. For malware detection we use the data trained in B2 period.\n\ndf_b2 = pd.read_feather(\"cached/v5_2.ft\")\n\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b2_p = cp.process(df_b2, f\"cached/v5_2.p3.{p3version}.cp.ft\")\n\n\npp_b2 = PreProcessing(normed, *skips_and_weights)\npp_b2.load_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\ndf_b2_pp, df_b2_pp_features, df_b2_pp_labels = pp_b2.process(\n    df_b2_p,\n    f\"cached/b2.p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=False  # use scaler trained in B1\n)\n\n\ndef predict(pp, clf, threshold, df_pp, df_orig):\n    \"\"\"\n    pp: PreProcessing object\n    clf: classifier\n    threshold: &gt;=threshold for positive (DoH) class\n    df_pp: preprocessed dataframe\n    df_orig: unprocessed dataframe\n    \"\"\"\n    df_pp2 = df_pp.copy()\n\n    df_pp2[\"IsDoHPredicted\"] = (clf.predict_proba(df_pp2[pp.feature_fields])[:,1] &gt;= threshold).astype(bool)\n\n    df_orig2 = df_orig.copy()\n    # fill predicted to the original dataframe\n    df_orig2[\"IsDoHPredicted\"] = df_pp2[\"IsDoHPredicted\"]\n    # non-443 rows will be NaN, replace with False\n    df_orig2[\"IsDoHPredicted\"] = df_orig2[\"IsDoHPredicted\"].fillna(False)\n\n    return df_orig2\n\n\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version}.ft\")\nresults_df[\"skips_and_weights\"] = results_df[\"skips_and_weights\"].apply(str)\nmodel, normed, skips_and_weights_str, auc, fprs, tprs, thresholds = results_df[\n    (results_df[\"model\"] == model) & (results_df[\"normed\"] == normed) & (results_df[\"skips_and_weights\"] == str(np.array(skips_and_weights)))\n].iloc[0]\n\n\ndata = {}\nfor sensitivity in [0.8, 0.9]:\n    threshold = [threshold for tpr, threshold in zip(tprs, thresholds) if tpr &gt;= sensitivity][0]\n    df_pred = predict(pp_b2, clf, threshold, df_b2_pp, df_b2)\n    data[sensitivity] = {\n        'df': df_pred,  # dataframe with predicted values,\n        'threshold': threshold,\n    }\n\n\ndef calc(df_n):\n    def rate(df_h):\n        dns_packets = df_h[df_h[\"IsDoHPredicted\"] == True][\"uint32 PACKETS\"].sum()  # outgoing packets\n        non_dns_ips = set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr SRC_IP\"].unique()).union(\n            set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr DST_IP\"].unique()))\n        rate = float(dns_packets) / (1.0 + len(non_dns_ips))\n        return {\"dns_packets\": dns_packets, \"non_dns\": len(non_dns_ips), \"rate\": rate, \"rate_log\": np.log(rate)}\n\n    return pd.DataFrame({ ip: rate(grp) for ip, grp in df_n.groupby([\"ipaddr SRC_IP\"]) }).T.reset_index()\n\n\n11.0.2.1 Calculate ratio on the whole dataset\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\n    display(calc(local_df))\n\n0.8\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n293700.0\n4260.0\n68.927482\n4.233055\n\n\n2\n192.168.2.217\n569784.0\n16.0\n33516.705882\n10.419799\n\n\n3\n192.168.2.249\n205116.0\n3398.0\n60.345984\n4.100094\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n0.9\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n349716.0\n3900.0\n89.647783\n4.495888\n\n\n2\n192.168.2.217\n802994.0\n14.0\n53532.933333\n10.888052\n\n\n3\n192.168.2.249\n266504.0\n3122.0\n85.335895\n4.446595\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n\n\n11.0.2.2 Split into time windows\n\nmin_ts = df_b2[\"time TIME_FIRST\"].apply(dateutil.parser.parse).min()\nmin_ts\n\nTimestamp('2023-03-23 14:50:04.316951')\n\n\n\nlocal_ips = df_b2[df_b2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")][\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips\n\n['192.168.2.249',\n '192.168.2.149',\n '192.168.2.42',\n '192.168.2.217',\n '192.168.1.1']\n\n\n\nf = '3min'\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")].copy()\n    local_df[\"time TIME_FIRST\"] = local_df[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n    \n    calc_ips = {}\n\n    a1_t = local_df.set_index([\"time TIME_FIRST\"])\n\n    df_resample = a1_t.groupby(pd.Grouper(freq=f, origin=min_ts)).first()\n    times = df_resample.index.tolist()\n    ranges = list(zip(times, times[1:]))\n\n    df_calcs = []\n    for ran in ranges:\n        df_ran = a1_t[(a1_t.index &gt; ran[0]) & (a1_t.index &lt; ran[1])]\n        df_calc = calc(df_ran)\n        df_calc[\"ts\"] = ran[0]\n        df_calcs.append(df_calc)\n\n    value[\"df_calc\"] = pd.concat(df_calcs).reset_index(drop=True)\n\n0.8\n\n\n0.9\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    fig, axs = plt.subplots(ncols=1, figsize=(12, 4))\n    df_calcs = value[\"df_calc\"]\n    for host in local_ips:\n        df_calc_host = df_calcs[df_calcs[\"index\"] == host].set_index(\"ts\")\n        df_calc_host[\"rate_log\"].plot(label=f'{host}', ax=axs)\n\n    plt.ylim(ymin=0)\n    plt.title(f\"Sensitivity: {sensitivity}, threshold: {value['threshold']}\")\n    plt.legend()\n    plt.show()\n\n0.8\n\n\n\n\n\n(a) sensitivity 0.8\n\n\n\n\n0.9\n\n\n\n\n\n(b) sensitivity 0.9\n\n\n\nFigure 11.1: \\(\\rho(a)\\) plot for all instances (B2 period)"
  },
  {
    "objectID": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-unnormed-0-0-0-0.html",
    "href": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-unnormed-0-0-0-0.html",
    "title": "12  Detect malware (RF unnormed-0-0-0-0)",
    "section": "",
    "text": "The model deployment is done in two stages: I. tune the model, II. turn on the malware detection (on the data collected in B1 and B2 periods respectively, described in the timeline Figure 3.1)\n\n\n\nTimeline\n\n\n\n12.0.1 Stage I: Model tuning\nOn this state we fit the scaler on the data collected in B1 period, to be used later on the second stage\n\nfrom detect_common import *\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\nmodel = 'RF'\nnormed = True\nskips_and_weights = [2,4,0,0]\n\n\n# Parameters\nmodel = \"RF\"\nnormed = False\nskips_and_weights = [0, 0, 0, 0]\n\n\nimport dateutil.parser\n\ndf_b1 = pd.read_feather(\"cached/v5_1.ft\")\n\n\npath_doh_ips_androids = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_general = \"doh_resolver_ip.csv\"\nfull_list = get_doh_ips(path_doh_ips_androids) + get_doh_ips(path_doh_ips_general)\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b1_p = cp.process(df_b1, f\"cached/v5_1.p3.{p3version}.cp.ft\")\n\n\nfrom joblib import dump, load\n\nname = (\n    ('normed-' if normed else 'unnormed-') + \n    '-'.join(str(p) for p in skips_and_weights)\n)\n\nclf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n\npp_b1 = PreProcessing(normed, *skips_and_weights)\n\ndf_b1_pp = pp_b1.process(\n    df_b1_p,\n    f\"cached/p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=True\n)\n\n\npp_b1.store_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\n\n\n\n12.0.2 Stage II: Malware detection\nOn this state we turn on the malware detection, provisioning the model with the scaler trained before. For malware detection we use the data trained in B2 period.\n\ndf_b2 = pd.read_feather(\"cached/v5_2.ft\")\n\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b2_p = cp.process(df_b2, f\"cached/v5_2.p3.{p3version}.cp.ft\")\n\n\npp_b2 = PreProcessing(normed, *skips_and_weights)\npp_b2.load_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\ndf_b2_pp, df_b2_pp_features, df_b2_pp_labels = pp_b2.process(\n    df_b2_p,\n    f\"cached/b2.p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=False  # use scaler trained in B1\n)\n\n\ndef predict(pp, clf, threshold, df_pp, df_orig):\n    \"\"\"\n    pp: PreProcessing object\n    clf: classifier\n    threshold: &gt;=threshold for positive (DoH) class\n    df_pp: preprocessed dataframe\n    df_orig: unprocessed dataframe\n    \"\"\"\n    df_pp2 = df_pp.copy()\n\n    df_pp2[\"IsDoHPredicted\"] = (clf.predict_proba(df_pp2[pp.feature_fields])[:,1] &gt;= threshold).astype(bool)\n\n    df_orig2 = df_orig.copy()\n    # fill predicted to the original dataframe\n    df_orig2[\"IsDoHPredicted\"] = df_pp2[\"IsDoHPredicted\"]\n    # non-443 rows will be NaN, replace with False\n    df_orig2[\"IsDoHPredicted\"] = df_orig2[\"IsDoHPredicted\"].fillna(False)\n\n    return df_orig2\n\n\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version}.ft\")\nresults_df[\"skips_and_weights\"] = results_df[\"skips_and_weights\"].apply(str)\nmodel, normed, skips_and_weights_str, auc, fprs, tprs, thresholds = results_df[\n    (results_df[\"model\"] == model) & (results_df[\"normed\"] == normed) & (results_df[\"skips_and_weights\"] == str(np.array(skips_and_weights)))\n].iloc[0]\n\n\ndata = {}\nfor sensitivity in [0.8, 0.9]:\n    threshold = [threshold for tpr, threshold in zip(tprs, thresholds) if tpr &gt;= sensitivity][0]\n    df_pred = predict(pp_b2, clf, threshold, df_b2_pp, df_b2)\n    data[sensitivity] = {\n        'df': df_pred,  # dataframe with predicted values,\n        'threshold': threshold,\n    }\n\n\ndef calc(df_n):\n    def rate(df_h):\n        dns_packets = df_h[df_h[\"IsDoHPredicted\"] == True][\"uint32 PACKETS\"].sum()  # outgoing packets\n        non_dns_ips = set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr SRC_IP\"].unique()).union(\n            set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr DST_IP\"].unique()))\n        rate = float(dns_packets) / (1.0 + len(non_dns_ips))\n        return {\"dns_packets\": dns_packets, \"non_dns\": len(non_dns_ips), \"rate\": rate, \"rate_log\": np.log(rate)}\n\n    return pd.DataFrame({ ip: rate(grp) for ip, grp in df_n.groupby([\"ipaddr SRC_IP\"]) }).T.reset_index()\n\n\n12.0.2.1 Calculate ratio on the whole dataset\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\n    display(calc(local_df))\n\n0.8\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n127048.0\n4905.0\n25.896453\n3.254106\n\n\n2\n192.168.2.217\n18589.0\n20.0\n885.190476\n6.785803\n\n\n3\n192.168.2.249\n39503.0\n3850.0\n10.257855\n2.328044\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n0.9\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n167503.0\n4797.0\n34.911005\n3.552802\n\n\n2\n192.168.2.217\n118691.0\n19.0\n5934.550000\n8.688546\n\n\n3\n192.168.2.249\n80540.0\n3761.0\n21.408825\n3.063803\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n\n\n12.0.2.2 Split into time windows\n\nmin_ts = df_b2[\"time TIME_FIRST\"].apply(dateutil.parser.parse).min()\nmin_ts\n\nTimestamp('2023-03-23 14:50:04.316951')\n\n\n\nlocal_ips = df_b2[df_b2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")][\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips\n\n['192.168.2.249',\n '192.168.2.149',\n '192.168.2.42',\n '192.168.2.217',\n '192.168.1.1']\n\n\n\nf = '3min'\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")].copy()\n    local_df[\"time TIME_FIRST\"] = local_df[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n    \n    calc_ips = {}\n\n    a1_t = local_df.set_index([\"time TIME_FIRST\"])\n\n    df_resample = a1_t.groupby(pd.Grouper(freq=f, origin=min_ts)).first()\n    times = df_resample.index.tolist()\n    ranges = list(zip(times, times[1:]))\n\n    df_calcs = []\n    for ran in ranges:\n        df_ran = a1_t[(a1_t.index &gt; ran[0]) & (a1_t.index &lt; ran[1])]\n        df_calc = calc(df_ran)\n        df_calc[\"ts\"] = ran[0]\n        df_calcs.append(df_calc)\n\n    value[\"df_calc\"] = pd.concat(df_calcs).reset_index(drop=True)\n\n0.8\n\n\n0.9\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    fig, axs = plt.subplots(ncols=1, figsize=(12, 4))\n    df_calcs = value[\"df_calc\"]\n    for host in local_ips:\n        df_calc_host = df_calcs[df_calcs[\"index\"] == host].set_index(\"ts\")\n        df_calc_host[\"rate_log\"].plot(label=f'{host}', ax=axs)\n\n    plt.ylim(ymin=0)\n    plt.title(f\"Sensitivity: {sensitivity}, threshold: {value['threshold']}\")\n    plt.legend()\n    plt.show()\n\n0.8\n\n\n\n\n\n(a) sensitivity 0.8\n\n\n\n\n0.9\n\n\n\n\n\n(b) sensitivity 0.9\n\n\n\nFigure 12.1: \\(\\rho(a)\\) plot for all instances (B2 period)"
  },
  {
    "objectID": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-normed-2-4-0-0.html",
    "href": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-normed-2-4-0-0.html",
    "title": "13  Detect malware (RF normed-2-4-0-0)",
    "section": "",
    "text": "The model deployment is done in two stages: I. tune the model, II. turn on the malware detection (on the data collected in B1 and B2 periods respectively, described in the timeline Figure 3.1)\n\n\n\nTimeline\n\n\n\n13.0.1 Stage I: Model tuning\nOn this state we fit the scaler on the data collected in B1 period, to be used later on the second stage\n\nfrom detect_common import *\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\nmodel = 'RF'\nnormed = True\nskips_and_weights = [2,4,0,0]\n\n\n# Parameters\nmodel = \"RF\"\nnormed = True\nskips_and_weights = [2, 4, 0, 0]\n\n\nimport dateutil.parser\n\ndf_b1 = pd.read_feather(\"cached/v5_1.ft\")\n\n\npath_doh_ips_androids = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_general = \"doh_resolver_ip.csv\"\nfull_list = get_doh_ips(path_doh_ips_androids) + get_doh_ips(path_doh_ips_general)\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b1_p = cp.process(df_b1, f\"cached/v5_1.p3.{p3version}.cp.ft\")\n\n\nfrom joblib import dump, load\n\nname = (\n    ('normed-' if normed else 'unnormed-') + \n    '-'.join(str(p) for p in skips_and_weights)\n)\n\nclf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n\npp_b1 = PreProcessing(normed, *skips_and_weights)\n\ndf_b1_pp = pp_b1.process(\n    df_b1_p,\n    f\"cached/p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=True\n)\n\n\npp_b1.store_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\n\n\n\n13.0.2 Stage II: Malware detection\nOn this state we turn on the malware detection, provisioning the model with the scaler trained before. For malware detection we use the data trained in B2 period.\n\ndf_b2 = pd.read_feather(\"cached/v5_2.ft\")\n\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b2_p = cp.process(df_b2, f\"cached/v5_2.p3.{p3version}.cp.ft\")\n\n\npp_b2 = PreProcessing(normed, *skips_and_weights)\npp_b2.load_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\ndf_b2_pp, df_b2_pp_features, df_b2_pp_labels = pp_b2.process(\n    df_b2_p,\n    f\"cached/b2.p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=False  # use scaler trained in B1\n)\n\n\ndef predict(pp, clf, threshold, df_pp, df_orig):\n    \"\"\"\n    pp: PreProcessing object\n    clf: classifier\n    threshold: &gt;=threshold for positive (DoH) class\n    df_pp: preprocessed dataframe\n    df_orig: unprocessed dataframe\n    \"\"\"\n    df_pp2 = df_pp.copy()\n\n    df_pp2[\"IsDoHPredicted\"] = (clf.predict_proba(df_pp2[pp.feature_fields])[:,1] &gt;= threshold).astype(bool)\n\n    df_orig2 = df_orig.copy()\n    # fill predicted to the original dataframe\n    df_orig2[\"IsDoHPredicted\"] = df_pp2[\"IsDoHPredicted\"]\n    # non-443 rows will be NaN, replace with False\n    df_orig2[\"IsDoHPredicted\"] = df_orig2[\"IsDoHPredicted\"].fillna(False)\n\n    return df_orig2\n\n\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version}.ft\")\nresults_df[\"skips_and_weights\"] = results_df[\"skips_and_weights\"].apply(str)\nmodel, normed, skips_and_weights_str, auc, fprs, tprs, thresholds = results_df[\n    (results_df[\"model\"] == model) & (results_df[\"normed\"] == normed) & (results_df[\"skips_and_weights\"] == str(np.array(skips_and_weights)))\n].iloc[0]\n\n\ndata = {}\nfor sensitivity in [0.8, 0.9]:\n    threshold = [threshold for tpr, threshold in zip(tprs, thresholds) if tpr &gt;= sensitivity][0]\n    df_pred = predict(pp_b2, clf, threshold, df_b2_pp, df_b2)\n    data[sensitivity] = {\n        'df': df_pred,  # dataframe with predicted values,\n        'threshold': threshold,\n    }\n\n\ndef calc(df_n):\n    def rate(df_h):\n        dns_packets = df_h[df_h[\"IsDoHPredicted\"] == True][\"uint32 PACKETS\"].sum()  # outgoing packets\n        non_dns_ips = set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr SRC_IP\"].unique()).union(\n            set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr DST_IP\"].unique()))\n        rate = float(dns_packets) / (1.0 + len(non_dns_ips))\n        return {\"dns_packets\": dns_packets, \"non_dns\": len(non_dns_ips), \"rate\": rate, \"rate_log\": np.log(rate)}\n\n    return pd.DataFrame({ ip: rate(grp) for ip, grp in df_n.groupby([\"ipaddr SRC_IP\"]) }).T.reset_index()\n\n\n13.0.2.1 Calculate ratio on the whole dataset\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\n    display(calc(local_df))\n\n0.8\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n230029.0\n4508.0\n51.015525\n3.932130\n\n\n2\n192.168.2.217\n334045.0\n17.0\n18558.055556\n9.828659\n\n\n3\n192.168.2.249\n140504.0\n3575.0\n39.290828\n3.670991\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n0.9\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n271287.0\n4303.0\n63.031366\n4.143632\n\n\n2\n192.168.2.217\n609350.0\n17.0\n33852.777778\n10.429776\n\n\n3\n192.168.2.249\n184519.0\n3414.0\n54.031918\n3.989575\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n\n\n13.0.2.2 Split into time windows\n\nmin_ts = df_b2[\"time TIME_FIRST\"].apply(dateutil.parser.parse).min()\nmin_ts\n\nTimestamp('2023-03-23 14:50:04.316951')\n\n\n\nlocal_ips = df_b2[df_b2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")][\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips\n\n['192.168.2.249',\n '192.168.2.149',\n '192.168.2.42',\n '192.168.2.217',\n '192.168.1.1']\n\n\n\nf = '3min'\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")].copy()\n    local_df[\"time TIME_FIRST\"] = local_df[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n    \n    calc_ips = {}\n\n    a1_t = local_df.set_index([\"time TIME_FIRST\"])\n\n    df_resample = a1_t.groupby(pd.Grouper(freq=f, origin=min_ts)).first()\n    times = df_resample.index.tolist()\n    ranges = list(zip(times, times[1:]))\n\n    df_calcs = []\n    for ran in ranges:\n        df_ran = a1_t[(a1_t.index &gt; ran[0]) & (a1_t.index &lt; ran[1])]\n        df_calc = calc(df_ran)\n        df_calc[\"ts\"] = ran[0]\n        df_calcs.append(df_calc)\n\n    value[\"df_calc\"] = pd.concat(df_calcs).reset_index(drop=True)\n\n0.8\n\n\n0.9\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    fig, axs = plt.subplots(ncols=1, figsize=(12, 4))\n    df_calcs = value[\"df_calc\"]\n    for host in local_ips:\n        df_calc_host = df_calcs[df_calcs[\"index\"] == host].set_index(\"ts\")\n        df_calc_host[\"rate_log\"].plot(label=f'{host}', ax=axs)\n\n    plt.ylim(ymin=0)\n    plt.title(f\"Sensitivity: {sensitivity}, threshold: {value['threshold']}\")\n    plt.legend()\n    plt.show()\n\n0.8\n\n\n\n\n\n(a) sensitivity 0.8\n\n\n\n\n0.9\n\n\n\n\n\n(b) sensitivity 0.9\n\n\n\nFigure 13.1: \\(\\rho(a)\\) plot for all instances (B2 period)"
  },
  {
    "objectID": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-normed-2-4-2-4.html",
    "href": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-normed-2-4-2-4.html",
    "title": "14  Detect malware (RF normed-2-4-2-4)",
    "section": "",
    "text": "The model deployment is done in two stages: I. tune the model, II. turn on the malware detection (on the data collected in B1 and B2 periods respectively, described in the timeline Figure 3.1)\n\n\n\nTimeline\n\n\n\n14.0.1 Stage I: Model tuning\nOn this state we fit the scaler on the data collected in B1 period, to be used later on the second stage\n\nfrom detect_common import *\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\nmodel = 'RF'\nnormed = True\nskips_and_weights = [2,4,0,0]\n\n\n# Parameters\nmodel = \"RF\"\nnormed = True\nskips_and_weights = [2, 4, 2, 4]\n\n\nimport dateutil.parser\n\ndf_b1 = pd.read_feather(\"cached/v5_1.ft\")\n\n\npath_doh_ips_androids = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_general = \"doh_resolver_ip.csv\"\nfull_list = get_doh_ips(path_doh_ips_androids) + get_doh_ips(path_doh_ips_general)\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b1_p = cp.process(df_b1, f\"cached/v5_1.p3.{p3version}.cp.ft\")\n\n\nfrom joblib import dump, load\n\nname = (\n    ('normed-' if normed else 'unnormed-') + \n    '-'.join(str(p) for p in skips_and_weights)\n)\n\nclf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n\npp_b1 = PreProcessing(normed, *skips_and_weights)\n\ndf_b1_pp = pp_b1.process(\n    df_b1_p,\n    f\"cached/p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=True\n)\n\n\npp_b1.store_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\n\n\n\n14.0.2 Stage II: Malware detection\nOn this state we turn on the malware detection, provisioning the model with the scaler trained before. For malware detection we use the data trained in B2 period.\n\ndf_b2 = pd.read_feather(\"cached/v5_2.ft\")\n\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b2_p = cp.process(df_b2, f\"cached/v5_2.p3.{p3version}.cp.ft\")\n\n\npp_b2 = PreProcessing(normed, *skips_and_weights)\npp_b2.load_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\ndf_b2_pp, df_b2_pp_features, df_b2_pp_labels = pp_b2.process(\n    df_b2_p,\n    f\"cached/b2.p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=False  # use scaler trained in B1\n)\n\n\ndef predict(pp, clf, threshold, df_pp, df_orig):\n    \"\"\"\n    pp: PreProcessing object\n    clf: classifier\n    threshold: &gt;=threshold for positive (DoH) class\n    df_pp: preprocessed dataframe\n    df_orig: unprocessed dataframe\n    \"\"\"\n    df_pp2 = df_pp.copy()\n\n    df_pp2[\"IsDoHPredicted\"] = (clf.predict_proba(df_pp2[pp.feature_fields])[:,1] &gt;= threshold).astype(bool)\n\n    df_orig2 = df_orig.copy()\n    # fill predicted to the original dataframe\n    df_orig2[\"IsDoHPredicted\"] = df_pp2[\"IsDoHPredicted\"]\n    # non-443 rows will be NaN, replace with False\n    df_orig2[\"IsDoHPredicted\"] = df_orig2[\"IsDoHPredicted\"].fillna(False)\n\n    return df_orig2\n\n\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version}.ft\")\nresults_df[\"skips_and_weights\"] = results_df[\"skips_and_weights\"].apply(str)\nmodel, normed, skips_and_weights_str, auc, fprs, tprs, thresholds = results_df[\n    (results_df[\"model\"] == model) & (results_df[\"normed\"] == normed) & (results_df[\"skips_and_weights\"] == str(np.array(skips_and_weights)))\n].iloc[0]\n\n\ndata = {}\nfor sensitivity in [0.8, 0.9]:\n    threshold = [threshold for tpr, threshold in zip(tprs, thresholds) if tpr &gt;= sensitivity][0]\n    df_pred = predict(pp_b2, clf, threshold, df_b2_pp, df_b2)\n    data[sensitivity] = {\n        'df': df_pred,  # dataframe with predicted values,\n        'threshold': threshold,\n    }\n\n\ndef calc(df_n):\n    def rate(df_h):\n        dns_packets = df_h[df_h[\"IsDoHPredicted\"] == True][\"uint32 PACKETS\"].sum()  # outgoing packets\n        non_dns_ips = set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr SRC_IP\"].unique()).union(\n            set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr DST_IP\"].unique()))\n        rate = float(dns_packets) / (1.0 + len(non_dns_ips))\n        return {\"dns_packets\": dns_packets, \"non_dns\": len(non_dns_ips), \"rate\": rate, \"rate_log\": np.log(rate)}\n\n    return pd.DataFrame({ ip: rate(grp) for ip, grp in df_n.groupby([\"ipaddr SRC_IP\"]) }).T.reset_index()\n\n\n14.0.2.1 Calculate ratio on the whole dataset\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\n    display(calc(local_df))\n\n0.8\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n220785.0\n4543.0\n48.588248\n3.883382\n\n\n2\n192.168.2.217\n330986.0\n21.0\n15044.818182\n9.618789\n\n\n3\n192.168.2.249\n136625.0\n3607.0\n37.867239\n3.634086\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n0.9\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n263891.0\n4339.0\n60.804378\n4.107662\n\n\n2\n192.168.2.217\n541214.0\n15.0\n33825.875000\n10.428981\n\n\n3\n192.168.2.249\n177432.0\n3445.0\n51.489263\n3.941373\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n\n\n14.0.2.2 Split into time windows\n\nmin_ts = df_b2[\"time TIME_FIRST\"].apply(dateutil.parser.parse).min()\nmin_ts\n\nTimestamp('2023-03-23 14:50:04.316951')\n\n\n\nlocal_ips = df_b2[df_b2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")][\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips\n\n['192.168.2.249',\n '192.168.2.149',\n '192.168.2.42',\n '192.168.2.217',\n '192.168.1.1']\n\n\n\nf = '3min'\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")].copy()\n    local_df[\"time TIME_FIRST\"] = local_df[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n    \n    calc_ips = {}\n\n    a1_t = local_df.set_index([\"time TIME_FIRST\"])\n\n    df_resample = a1_t.groupby(pd.Grouper(freq=f, origin=min_ts)).first()\n    times = df_resample.index.tolist()\n    ranges = list(zip(times, times[1:]))\n\n    df_calcs = []\n    for ran in ranges:\n        df_ran = a1_t[(a1_t.index &gt; ran[0]) & (a1_t.index &lt; ran[1])]\n        df_calc = calc(df_ran)\n        df_calc[\"ts\"] = ran[0]\n        df_calcs.append(df_calc)\n\n    value[\"df_calc\"] = pd.concat(df_calcs).reset_index(drop=True)\n\n0.8\n\n\n0.9\n\n\n\nimport matplotlib.pyplot as plt\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    fig, axs = plt.subplots(ncols=1, figsize=(12, 4))\n    df_calcs = value[\"df_calc\"]\n    for host in local_ips:\n        df_calc_host = df_calcs[df_calcs[\"index\"] == host].set_index(\"ts\")\n        df_calc_host[\"rate_log\"].plot(label=f'{host}', ax=axs)\n\n    plt.ylim(ymin=0)\n    plt.title(f\"Sensitivity: {sensitivity}, threshold: {value['threshold']}\")\n    plt.legend()\n    plt.show()\n\n0.8\n\n\n\n\n\n0.9"
  },
  {
    "objectID": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-normed-1-0-0-0.html",
    "href": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-normed-1-0-0-0.html",
    "title": "15  Detect malware (RF normed-1-0-0-0)",
    "section": "",
    "text": "The model deployment is done in two stages: I. tune the model, II. turn on the malware detection (on the data collected in B1 and B2 periods respectively, described in the timeline Figure 3.1)\n\n\n\nTimeline\n\n\n\n15.0.1 Stage I: Model tuning\nOn this state we fit the scaler on the data collected in B1 period, to be used later on the second stage\n\nfrom detect_common import *\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\nmodel = 'RF'\nnormed = True\nskips_and_weights = [2,4,0,0]\n\n\n# Parameters\nmodel = \"RF\"\nnormed = True\nskips_and_weights = [1, 0, 0, 0]\n\n\nimport dateutil.parser\n\ndf_b1 = pd.read_feather(\"cached/v5_1.ft\")\n\n\npath_doh_ips_androids = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_general = \"doh_resolver_ip.csv\"\nfull_list = get_doh_ips(path_doh_ips_androids) + get_doh_ips(path_doh_ips_general)\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b1_p = cp.process(df_b1, f\"cached/v5_1.p3.{p3version}.cp.ft\")\n\n\nfrom joblib import dump, load\n\nname = (\n    ('normed-' if normed else 'unnormed-') + \n    '-'.join(str(p) for p in skips_and_weights)\n)\n\nclf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n\npp_b1 = PreProcessing(normed, *skips_and_weights)\n\ndf_b1_pp = pp_b1.process(\n    df_b1_p,\n    f\"cached/p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=True\n)\n\n\npp_b1.store_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\n\n\n\n15.0.2 Stage II: Malware detection\nOn this state we turn on the malware detection, provisioning the model with the scaler trained before. For malware detection we use the data trained in B2 period.\n\ndf_b2 = pd.read_feather(\"cached/v5_2.ft\")\n\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b2_p = cp.process(df_b2, f\"cached/v5_2.p3.{p3version}.cp.ft\")\n\n\npp_b2 = PreProcessing(normed, *skips_and_weights)\npp_b2.load_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\ndf_b2_pp, df_b2_pp_features, df_b2_pp_labels = pp_b2.process(\n    df_b2_p,\n    f\"cached/b2.p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=False  # use scaler trained in B1\n)\n\n\ndef predict(pp, clf, threshold, df_pp, df_orig):\n    \"\"\"\n    pp: PreProcessing object\n    clf: classifier\n    threshold: &gt;=threshold for positive (DoH) class\n    df_pp: preprocessed dataframe\n    df_orig: unprocessed dataframe\n    \"\"\"\n    df_pp2 = df_pp.copy()\n\n    df_pp2[\"IsDoHPredicted\"] = (clf.predict_proba(df_pp2[pp.feature_fields])[:,1] &gt;= threshold).astype(bool)\n\n    df_orig2 = df_orig.copy()\n    # fill predicted to the original dataframe\n    df_orig2[\"IsDoHPredicted\"] = df_pp2[\"IsDoHPredicted\"]\n    # non-443 rows will be NaN, replace with False\n    df_orig2[\"IsDoHPredicted\"] = df_orig2[\"IsDoHPredicted\"].fillna(False)\n\n    return df_orig2\n\n\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version}.ft\")\nresults_df[\"skips_and_weights\"] = results_df[\"skips_and_weights\"].apply(str)\nmodel, normed, skips_and_weights_str, auc, fprs, tprs, thresholds = results_df[\n    (results_df[\"model\"] == model) & (results_df[\"normed\"] == normed) & (results_df[\"skips_and_weights\"] == str(np.array(skips_and_weights)))\n].iloc[0]\n\n\ndata = {}\nfor sensitivity in [0.8, 0.9]:\n    threshold = [threshold for tpr, threshold in zip(tprs, thresholds) if tpr &gt;= sensitivity][0]\n    df_pred = predict(pp_b2, clf, threshold, df_b2_pp, df_b2)\n    data[sensitivity] = {\n        'df': df_pred,  # dataframe with predicted values,\n        'threshold': threshold,\n    }\n\n\ndef calc(df_n):\n    def rate(df_h):\n        dns_packets = df_h[df_h[\"IsDoHPredicted\"] == True][\"uint32 PACKETS\"].sum()  # outgoing packets\n        non_dns_ips = set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr SRC_IP\"].unique()).union(\n            set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr DST_IP\"].unique()))\n        rate = float(dns_packets) / (1.0 + len(non_dns_ips))\n        return {\"dns_packets\": dns_packets, \"non_dns\": len(non_dns_ips), \"rate\": rate, \"rate_log\": np.log(rate)}\n\n    return pd.DataFrame({ ip: rate(grp) for ip, grp in df_n.groupby([\"ipaddr SRC_IP\"]) }).T.reset_index()\n\n\n15.0.2.1 Calculate ratio on the whole dataset\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\n    display(calc(local_df))\n\n0.8\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.00000\n-inf\n\n\n1\n192.168.2.149\n273854.0\n4415.0\n62.01404\n4.127361\n\n\n2\n192.168.2.217\n96561.0\n17.0\n5364.50000\n8.587558\n\n\n3\n192.168.2.249\n190943.0\n3532.0\n54.04557\n3.989828\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.00000\n-inf\n\n\n\n\n\n\n\n0.9\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n354693.0\n3953.0\n89.704856\n4.496525\n\n\n2\n192.168.2.217\n699158.0\n15.0\n43697.375000\n10.685043\n\n\n3\n192.168.2.249\n264365.0\n3179.0\n83.133648\n4.420450\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n\n\n15.0.2.2 Split into time windows\n\nmin_ts = df_b2[\"time TIME_FIRST\"].apply(dateutil.parser.parse).min()\nmin_ts\n\nTimestamp('2023-03-23 14:50:04.316951')\n\n\n\nlocal_ips = df_b2[df_b2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")][\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips\n\n['192.168.2.249',\n '192.168.2.149',\n '192.168.2.42',\n '192.168.2.217',\n '192.168.1.1']\n\n\n\nf = '3min'\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")].copy()\n    local_df[\"time TIME_FIRST\"] = local_df[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n    \n    calc_ips = {}\n\n    a1_t = local_df.set_index([\"time TIME_FIRST\"])\n\n    df_resample = a1_t.groupby(pd.Grouper(freq=f, origin=min_ts)).first()\n    times = df_resample.index.tolist()\n    ranges = list(zip(times, times[1:]))\n\n    df_calcs = []\n    for ran in ranges:\n        df_ran = a1_t[(a1_t.index &gt; ran[0]) & (a1_t.index &lt; ran[1])]\n        df_calc = calc(df_ran)\n        df_calc[\"ts\"] = ran[0]\n        df_calcs.append(df_calc)\n\n    value[\"df_calc\"] = pd.concat(df_calcs).reset_index(drop=True)\n\n0.8\n\n\n0.9\n\n\n\nimport matplotlib.pyplot as plt\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    fig, axs = plt.subplots(ncols=1, figsize=(12, 4))\n    df_calcs = value[\"df_calc\"]\n    for host in local_ips:\n        df_calc_host = df_calcs[df_calcs[\"index\"] == host].set_index(\"ts\")\n        df_calc_host[\"rate_log\"].plot(label=f'{host}', ax=axs)\n\n    plt.ylim(ymin=0)\n    plt.title(f\"Sensitivity: {sensitivity}, threshold: {value['threshold']}\")\n    plt.legend()\n    plt.show()\n\n0.8\n\n\n\n\n\n0.9"
  },
  {
    "objectID": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-unnormed-2-0-0-0.html",
    "href": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-unnormed-2-0-0-0.html",
    "title": "16  Detect malware (RF unnormed-2-0-0-0)",
    "section": "",
    "text": "The model deployment is done in two stages: I. tune the model, II. turn on the malware detection (on the data collected in B1 and B2 periods respectively, described in the timeline Figure 3.1)\n\n\n\nTimeline\n\n\n\n16.0.1 Stage I: Model tuning\nOn this state we fit the scaler on the data collected in B1 period, to be used later on the second stage\n\nfrom detect_common import *\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\nmodel = 'RF'\nnormed = True\nskips_and_weights = [2,4,0,0]\n\n\n# Parameters\nmodel = \"RF\"\nnormed = False\nskips_and_weights = [2, 0, 0, 0]\n\n\nimport dateutil.parser\n\ndf_b1 = pd.read_feather(\"cached/v5_1.ft\")\n\n\npath_doh_ips_androids = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_general = \"doh_resolver_ip.csv\"\nfull_list = get_doh_ips(path_doh_ips_androids) + get_doh_ips(path_doh_ips_general)\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b1_p = cp.process(df_b1, f\"cached/v5_1.p3.{p3version}.cp.ft\")\n\n\nfrom joblib import dump, load\n\nname = (\n    ('normed-' if normed else 'unnormed-') + \n    '-'.join(str(p) for p in skips_and_weights)\n)\n\nclf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n\npp_b1 = PreProcessing(normed, *skips_and_weights)\n\ndf_b1_pp = pp_b1.process(\n    df_b1_p,\n    f\"cached/p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=True\n)\n\n\npp_b1.store_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\n\n\n\n16.0.2 Stage II: Malware detection\nOn this state we turn on the malware detection, provisioning the model with the scaler trained before. For malware detection we use the data trained in B2 period.\n\ndf_b2 = pd.read_feather(\"cached/v5_2.ft\")\n\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b2_p = cp.process(df_b2, f\"cached/v5_2.p3.{p3version}.cp.ft\")\n\n\npp_b2 = PreProcessing(normed, *skips_and_weights)\npp_b2.load_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\ndf_b2_pp, df_b2_pp_features, df_b2_pp_labels = pp_b2.process(\n    df_b2_p,\n    f\"cached/b2.p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=False  # use scaler trained in B1\n)\n\n\ndef predict(pp, clf, threshold, df_pp, df_orig):\n    \"\"\"\n    pp: PreProcessing object\n    clf: classifier\n    threshold: &gt;=threshold for positive (DoH) class\n    df_pp: preprocessed dataframe\n    df_orig: unprocessed dataframe\n    \"\"\"\n    df_pp2 = df_pp.copy()\n\n    df_pp2[\"IsDoHPredicted\"] = (clf.predict_proba(df_pp2[pp.feature_fields])[:,1] &gt;= threshold).astype(bool)\n\n    df_orig2 = df_orig.copy()\n    # fill predicted to the original dataframe\n    df_orig2[\"IsDoHPredicted\"] = df_pp2[\"IsDoHPredicted\"]\n    # non-443 rows will be NaN, replace with False\n    df_orig2[\"IsDoHPredicted\"] = df_orig2[\"IsDoHPredicted\"].fillna(False)\n\n    return df_orig2\n\n\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version}.ft\")\nresults_df[\"skips_and_weights\"] = results_df[\"skips_and_weights\"].apply(str)\nmodel, normed, skips_and_weights_str, auc, fprs, tprs, thresholds = results_df[\n    (results_df[\"model\"] == model) & (results_df[\"normed\"] == normed) & (results_df[\"skips_and_weights\"] == str(np.array(skips_and_weights)))\n].iloc[0]\n\n\ndata = {}\nfor sensitivity in [0.8, 0.9]:\n    threshold = [threshold for tpr, threshold in zip(tprs, thresholds) if tpr &gt;= sensitivity][0]\n    df_pred = predict(pp_b2, clf, threshold, df_b2_pp, df_b2)\n    data[sensitivity] = {\n        'df': df_pred,  # dataframe with predicted values,\n        'threshold': threshold,\n    }\n\n\ndef calc(df_n):\n    def rate(df_h):\n        dns_packets = df_h[df_h[\"IsDoHPredicted\"] == True][\"uint32 PACKETS\"].sum()  # outgoing packets\n        non_dns_ips = set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr SRC_IP\"].unique()).union(\n            set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr DST_IP\"].unique()))\n        rate = float(dns_packets) / (1.0 + len(non_dns_ips))\n        return {\"dns_packets\": dns_packets, \"non_dns\": len(non_dns_ips), \"rate\": rate, \"rate_log\": np.log(rate)}\n\n    return pd.DataFrame({ ip: rate(grp) for ip, grp in df_n.groupby([\"ipaddr SRC_IP\"]) }).T.reset_index()\n\n\n16.0.2.1 Calculate ratio on the whole dataset\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\n    display(calc(local_df))\n\n0.8\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n95366.0\n4935.0\n19.320502\n2.961167\n\n\n2\n192.168.2.217\n573.0\n19.0\n28.650000\n3.355153\n\n\n3\n192.168.2.249\n24721.0\n3870.0\n6.386205\n1.854140\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n0.9\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n120731.0\n4901.0\n24.628927\n3.203922\n\n\n2\n192.168.2.217\n24193.0\n19.0\n1209.650000\n7.098086\n\n\n3\n192.168.2.249\n41263.0\n3856.0\n10.698211\n2.370077\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n\n\n16.0.2.2 Split into time windows\n\nmin_ts = df_b2[\"time TIME_FIRST\"].apply(dateutil.parser.parse).min()\nmin_ts\n\nTimestamp('2023-03-23 14:50:04.316951')\n\n\n\nlocal_ips = df_b2[df_b2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")][\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips\n\n['192.168.2.249',\n '192.168.2.149',\n '192.168.2.42',\n '192.168.2.217',\n '192.168.1.1']\n\n\n\nf = '3min'\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")].copy()\n    local_df[\"time TIME_FIRST\"] = local_df[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n    \n    calc_ips = {}\n\n    a1_t = local_df.set_index([\"time TIME_FIRST\"])\n\n    df_resample = a1_t.groupby(pd.Grouper(freq=f, origin=min_ts)).first()\n    times = df_resample.index.tolist()\n    ranges = list(zip(times, times[1:]))\n\n    df_calcs = []\n    for ran in ranges:\n        df_ran = a1_t[(a1_t.index &gt; ran[0]) & (a1_t.index &lt; ran[1])]\n        df_calc = calc(df_ran)\n        df_calc[\"ts\"] = ran[0]\n        df_calcs.append(df_calc)\n\n    value[\"df_calc\"] = pd.concat(df_calcs).reset_index(drop=True)\n\n0.8\n\n\n0.9\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    fig, axs = plt.subplots(ncols=1, figsize=(12, 4))\n    df_calcs = value[\"df_calc\"]\n    for host in local_ips:\n        df_calc_host = df_calcs[df_calcs[\"index\"] == host].set_index(\"ts\")\n        df_calc_host[\"rate_log\"].plot(label=f'{host}', ax=axs)\n\n    plt.ylim(ymin=0)\n    plt.title(f\"Sensitivity: {sensitivity}, threshold: {value['threshold']}\")\n    plt.legend()\n    plt.show()\n\n0.8\n\n\n\n\n\n(a) sensitivity 0.8\n\n\n\n\n0.9\n\n\n\n\n\n(b) sensitivity 0.9\n\n\n\nFigure 16.1: \\(\\rho(a)\\) plot for all instances (B2 period)"
  },
  {
    "objectID": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-unnormed-2-4-0-0.html",
    "href": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-unnormed-2-4-0-0.html",
    "title": "17  Detect malware (RF unnormed-2-4-0-0)",
    "section": "",
    "text": "The model deployment is done in two stages: I. tune the model, II. turn on the malware detection (on the data collected in B1 and B2 periods respectively, described in the timeline Figure 3.1)\n\n\n\nTimeline\n\n\n\n17.0.1 Stage I: Model tuning\nOn this state we fit the scaler on the data collected in B1 period, to be used later on the second stage\n\nfrom detect_common import *\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\nmodel = 'RF'\nnormed = True\nskips_and_weights = [2,4,0,0]\n\n\n# Parameters\nmodel = \"RF\"\nnormed = False\nskips_and_weights = [2, 4, 0, 0]\n\n\nimport dateutil.parser\n\ndf_b1 = pd.read_feather(\"cached/v5_1.ft\")\n\n\npath_doh_ips_androids = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_general = \"doh_resolver_ip.csv\"\nfull_list = get_doh_ips(path_doh_ips_androids) + get_doh_ips(path_doh_ips_general)\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b1_p = cp.process(df_b1, f\"cached/v5_1.p3.{p3version}.cp.ft\")\n\n\nfrom joblib import dump, load\n\nname = (\n    ('normed-' if normed else 'unnormed-') + \n    '-'.join(str(p) for p in skips_and_weights)\n)\n\nclf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n\npp_b1 = PreProcessing(normed, *skips_and_weights)\n\ndf_b1_pp = pp_b1.process(\n    df_b1_p,\n    f\"cached/p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=True\n)\n\n\npp_b1.store_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\n\n\n\n17.0.2 Stage II: Malware detection\nOn this state we turn on the malware detection, provisioning the model with the scaler trained before. For malware detection we use the data trained in B2 period.\n\ndf_b2 = pd.read_feather(\"cached/v5_2.ft\")\n\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b2_p = cp.process(df_b2, f\"cached/v5_2.p3.{p3version}.cp.ft\")\n\n\npp_b2 = PreProcessing(normed, *skips_and_weights)\npp_b2.load_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\ndf_b2_pp, df_b2_pp_features, df_b2_pp_labels = pp_b2.process(\n    df_b2_p,\n    f\"cached/b2.p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=False  # use scaler trained in B1\n)\n\n\ndef predict(pp, clf, threshold, df_pp, df_orig):\n    \"\"\"\n    pp: PreProcessing object\n    clf: classifier\n    threshold: &gt;=threshold for positive (DoH) class\n    df_pp: preprocessed dataframe\n    df_orig: unprocessed dataframe\n    \"\"\"\n    df_pp2 = df_pp.copy()\n\n    df_pp2[\"IsDoHPredicted\"] = (clf.predict_proba(df_pp2[pp.feature_fields])[:,1] &gt;= threshold).astype(bool)\n\n    df_orig2 = df_orig.copy()\n    # fill predicted to the original dataframe\n    df_orig2[\"IsDoHPredicted\"] = df_pp2[\"IsDoHPredicted\"]\n    # non-443 rows will be NaN, replace with False\n    df_orig2[\"IsDoHPredicted\"] = df_orig2[\"IsDoHPredicted\"].fillna(False)\n\n    return df_orig2\n\n\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version}.ft\")\nresults_df[\"skips_and_weights\"] = results_df[\"skips_and_weights\"].apply(str)\nmodel, normed, skips_and_weights_str, auc, fprs, tprs, thresholds = results_df[\n    (results_df[\"model\"] == model) & (results_df[\"normed\"] == normed) & (results_df[\"skips_and_weights\"] == str(np.array(skips_and_weights)))\n].iloc[0]\n\n\ndata = {}\nfor sensitivity in [0.8, 0.9]:\n    threshold = [threshold for tpr, threshold in zip(tprs, thresholds) if tpr &gt;= sensitivity][0]\n    df_pred = predict(pp_b2, clf, threshold, df_b2_pp, df_b2)\n    data[sensitivity] = {\n        'df': df_pred,  # dataframe with predicted values,\n        'threshold': threshold,\n    }\n\n\ndef calc(df_n):\n    def rate(df_h):\n        dns_packets = df_h[df_h[\"IsDoHPredicted\"] == True][\"uint32 PACKETS\"].sum()  # outgoing packets\n        non_dns_ips = set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr SRC_IP\"].unique()).union(\n            set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr DST_IP\"].unique()))\n        rate = float(dns_packets) / (1.0 + len(non_dns_ips))\n        return {\"dns_packets\": dns_packets, \"non_dns\": len(non_dns_ips), \"rate\": rate, \"rate_log\": np.log(rate)}\n\n    return pd.DataFrame({ ip: rate(grp) for ip, grp in df_n.groupby([\"ipaddr SRC_IP\"]) }).T.reset_index()\n\n\n17.0.2.1 Calculate ratio on the whole dataset\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\n    display(calc(local_df))\n\n0.8\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n93503.0\n4930.0\n18.962279\n2.942452\n\n\n2\n192.168.2.217\n141.0\n21.0\n6.409091\n1.857717\n\n\n3\n192.168.2.249\n21203.0\n3872.0\n5.474568\n1.700113\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n0.9\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n136344.0\n4861.0\n28.042781\n3.333731\n\n\n2\n192.168.2.217\n4590.0\n19.0\n229.500000\n5.435903\n\n\n3\n192.168.2.249\n55013.0\n3833.0\n14.348722\n2.663661\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n\n\n17.0.2.2 Split into time windows\n\nmin_ts = df_b2[\"time TIME_FIRST\"].apply(dateutil.parser.parse).min()\nmin_ts\n\nTimestamp('2023-03-23 14:50:04.316951')\n\n\n\nlocal_ips = df_b2[df_b2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")][\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips\n\n['192.168.2.249',\n '192.168.2.149',\n '192.168.2.42',\n '192.168.2.217',\n '192.168.1.1']\n\n\n\nf = '3min'\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")].copy()\n    local_df[\"time TIME_FIRST\"] = local_df[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n    \n    calc_ips = {}\n\n    a1_t = local_df.set_index([\"time TIME_FIRST\"])\n\n    df_resample = a1_t.groupby(pd.Grouper(freq=f, origin=min_ts)).first()\n    times = df_resample.index.tolist()\n    ranges = list(zip(times, times[1:]))\n\n    df_calcs = []\n    for ran in ranges:\n        df_ran = a1_t[(a1_t.index &gt; ran[0]) & (a1_t.index &lt; ran[1])]\n        df_calc = calc(df_ran)\n        df_calc[\"ts\"] = ran[0]\n        df_calcs.append(df_calc)\n\n    value[\"df_calc\"] = pd.concat(df_calcs).reset_index(drop=True)\n\n0.8\n\n\n0.9\n\n\n\nimport matplotlib.pyplot as plt\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    fig, axs = plt.subplots(ncols=1, figsize=(12, 4))\n    df_calcs = value[\"df_calc\"]\n    for host in local_ips:\n        df_calc_host = df_calcs[df_calcs[\"index\"] == host].set_index(\"ts\")\n        df_calc_host[\"rate_log\"].plot(label=f'{host}', ax=axs)\n\n    plt.ylim(ymin=0)\n    plt.title(f\"Sensitivity: {sensitivity}, threshold: {value['threshold']}\")\n    plt.legend()\n    plt.show()\n\n0.8\n\n\n\n\n\n0.9"
  },
  {
    "objectID": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-unnormed-1-4-1-4.html",
    "href": "notebooks/flubot-detection-try-models/gen-flubot-detection-RF-unnormed-1-4-1-4.html",
    "title": "18  Detect malware (RF unnormed-1-4-1-4)",
    "section": "",
    "text": "The model deployment is done in two stages: I. tune the model, II. turn on the malware detection (on the data collected in B1 and B2 periods respectively, described in the timeline Figure 3.1)\n\n\n\nTimeline\n\n\n\n18.0.1 Stage I: Model tuning\nOn this state we fit the scaler on the data collected in B1 period, to be used later on the second stage\n\nfrom detect_common import *\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v5\"\n\n\nmodel = 'RF'\nnormed = True\nskips_and_weights = [2,4,0,0]\n\n\n# Parameters\nmodel = \"RF\"\nnormed = False\nskips_and_weights = [1, 4, 1, 4]\n\n\nimport dateutil.parser\n\ndf_b1 = pd.read_feather(\"cached/v5_1.ft\")\n\n\npath_doh_ips_androids = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_general = \"doh_resolver_ip.csv\"\nfull_list = get_doh_ips(path_doh_ips_androids) + get_doh_ips(path_doh_ips_general)\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b1_p = cp.process(df_b1, f\"cached/v5_1.p3.{p3version}.cp.ft\")\n\n\nfrom joblib import dump, load\n\nname = (\n    ('normed-' if normed else 'unnormed-') + \n    '-'.join(str(p) for p in skips_and_weights)\n)\n\nclf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\n\n\npp_b1 = PreProcessing(normed, *skips_and_weights)\n\ndf_b1_pp = pp_b1.process(\n    df_b1_p,\n    f\"cached/p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=True\n)\n\n\npp_b1.store_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\n\n\n\n18.0.2 Stage II: Malware detection\nOn this state we turn on the malware detection, provisioning the model with the scaler trained before. For malware detection we use the data trained in B2 period.\n\ndf_b2 = pd.read_feather(\"cached/v5_2.ft\")\n\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b2_p = cp.process(df_b2, f\"cached/v5_2.p3.{p3version}.cp.ft\")\n\n\npp_b2 = PreProcessing(normed, *skips_and_weights)\npp_b2.load_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\ndf_b2_pp, df_b2_pp_features, df_b2_pp_labels = pp_b2.process(\n    df_b2_p,\n    f\"cached/b2.p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=False  # use scaler trained in B1\n)\n\n\ndef predict(pp, clf, threshold, df_pp, df_orig):\n    \"\"\"\n    pp: PreProcessing object\n    clf: classifier\n    threshold: &gt;=threshold for positive (DoH) class\n    df_pp: preprocessed dataframe\n    df_orig: unprocessed dataframe\n    \"\"\"\n    df_pp2 = df_pp.copy()\n\n    df_pp2[\"IsDoHPredicted\"] = (clf.predict_proba(df_pp2[pp.feature_fields])[:,1] &gt;= threshold).astype(bool)\n\n    df_orig2 = df_orig.copy()\n    # fill predicted to the original dataframe\n    df_orig2[\"IsDoHPredicted\"] = df_pp2[\"IsDoHPredicted\"]\n    # non-443 rows will be NaN, replace with False\n    df_orig2[\"IsDoHPredicted\"] = df_orig2[\"IsDoHPredicted\"].fillna(False)\n\n    return df_orig2\n\n\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version}.ft\")\nresults_df[\"skips_and_weights\"] = results_df[\"skips_and_weights\"].apply(str)\nmodel, normed, skips_and_weights_str, auc, fprs, tprs, thresholds = results_df[\n    (results_df[\"model\"] == model) & (results_df[\"normed\"] == normed) & (results_df[\"skips_and_weights\"] == str(np.array(skips_and_weights)))\n].iloc[0]\n\n\ndata = {}\nfor sensitivity in [0.8, 0.9]:\n    threshold = [threshold for tpr, threshold in zip(tprs, thresholds) if tpr &gt;= sensitivity][0]\n    df_pred = predict(pp_b2, clf, threshold, df_b2_pp, df_b2)\n    data[sensitivity] = {\n        'df': df_pred,  # dataframe with predicted values,\n        'threshold': threshold,\n    }\n\n\ndef calc(df_n):\n    def rate(df_h):\n        dns_packets = df_h[df_h[\"IsDoHPredicted\"] == True][\"uint32 PACKETS\"].sum()  # outgoing packets\n        non_dns_ips = set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr SRC_IP\"].unique()).union(\n            set(df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr DST_IP\"].unique()))\n        rate = float(dns_packets) / (1.0 + len(non_dns_ips))\n        return {\"dns_packets\": dns_packets, \"non_dns\": len(non_dns_ips), \"rate\": rate, \"rate_log\": np.log(rate)}\n\n    return pd.DataFrame({ ip: rate(grp) for ip, grp in df_n.groupby([\"ipaddr SRC_IP\"]) }).T.reset_index()\n\n\n18.0.2.1 Calculate ratio on the whole dataset\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\n    display(calc(local_df))\n\n0.8\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n110756.0\n4912.0\n22.543456\n3.115445\n\n\n2\n192.168.2.217\n235.0\n21.0\n10.681818\n2.368543\n\n\n3\n192.168.2.249\n26635.0\n3865.0\n6.889550\n1.930006\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n0.9\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n133846.0\n4854.0\n27.568692\n3.316681\n\n\n2\n192.168.2.217\n4620.0\n19.0\n231.000000\n5.442418\n\n\n3\n192.168.2.249\n48900.0\n3836.0\n12.744332\n2.545087\n\n\n4\n192.168.2.42\n0.0\n5.0\n0.000000\n-inf\n\n\n\n\n\n\n\n\n\n18.0.2.2 Split into time windows\n\nmin_ts = df_b2[\"time TIME_FIRST\"].apply(dateutil.parser.parse).min()\nmin_ts\n\nTimestamp('2023-03-23 14:50:04.316951')\n\n\n\nlocal_ips = df_b2[df_b2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")][\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips\n\n['192.168.2.249',\n '192.168.2.149',\n '192.168.2.42',\n '192.168.2.217',\n '192.168.1.1']\n\n\n\nf = '3min'\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")].copy()\n    local_df[\"time TIME_FIRST\"] = local_df[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n    \n    calc_ips = {}\n\n    a1_t = local_df.set_index([\"time TIME_FIRST\"])\n\n    df_resample = a1_t.groupby(pd.Grouper(freq=f, origin=min_ts)).first()\n    times = df_resample.index.tolist()\n    ranges = list(zip(times, times[1:]))\n\n    df_calcs = []\n    for ran in ranges:\n        df_ran = a1_t[(a1_t.index &gt; ran[0]) & (a1_t.index &lt; ran[1])]\n        df_calc = calc(df_ran)\n        df_calc[\"ts\"] = ran[0]\n        df_calcs.append(df_calc)\n\n    value[\"df_calc\"] = pd.concat(df_calcs).reset_index(drop=True)\n\n0.8\n\n\n0.9\n\n\n\nimport matplotlib.pyplot as plt\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    fig, axs = plt.subplots(ncols=1, figsize=(12, 4))\n    df_calcs = value[\"df_calc\"]\n    for host in local_ips:\n        df_calc_host = df_calcs[df_calcs[\"index\"] == host].set_index(\"ts\")\n        df_calc_host[\"rate_log\"].plot(label=f'{host}', ax=axs)\n\n    plt.ylim(ymin=0)\n    plt.title(f\"Sensitivity: {sensitivity}, threshold: {value['threshold']}\")\n    plt.legend()\n    plt.show()\n\n0.8\n\n\n\n\n\n0.9"
  },
  {
    "objectID": "flubot-detection-2.html",
    "href": "flubot-detection-2.html",
    "title": "Experiment 2: Real-world scenario",
    "section": "",
    "text": "In this part we run the malware detection in the real-world scenario, where a single Android machine is first DoH benign traffic\nIn this experiment we have 2 Android x86 machines:\n\n192.168.2.217 - machine with mixed traffic, benign in A1 period; benign and infected with FluBot in A2 period (see Figure 3.1)\n192.168.2.149 - clean machine with DoH in Chrome enabled"
  },
  {
    "objectID": "flubot-detection-2-overview.html#network-setup",
    "href": "flubot-detection-2-overview.html#network-setup",
    "title": "19  Design and log of experiment 2",
    "section": "19.1 Network setup",
    "text": "19.1 Network setup\nWe configured 2 Android x86 machines:\n\n192.168.2.217 - machine with mixed traffic, benign in A1 period; benign and infected with FluBot in A2 period (see Figure 3.1)\n192.168.2.149 - clean machine with DoH in Chrome enabled\n\nAlso, same as in the first experiment, we have:\n\nOpenWRT router of sandbox network with IPFIXprobe running\nLinux machine inside the sandbox network, which is used to control Android machines via ADB shell\nLinux machine outside of the network which is used to collect traffic from IPFIXprobe and convert it to CSV"
  },
  {
    "objectID": "flubot-detection-2-overview.html#experiment-plan",
    "href": "flubot-detection-2-overview.html#experiment-plan",
    "title": "19  Design and log of experiment 2",
    "section": "19.2 Experiment plan",
    "text": "19.2 Experiment plan\nWe conduct an experiment with the plan similar to the first one, collecting data in two (B1 and B2) periods (see Figure 3.1).\n\n\n\nTimeline\n\n\n\nB1: We collect a sample of traffic from the monitored network to train the scaler (see Section 10.1)\nB2: We collect benign (A1) and infected (A2) traffic to validate the malware detector"
  },
  {
    "objectID": "flubot-detection-2-overview.html#log",
    "href": "flubot-detection-2-overview.html#log",
    "title": "19  Design and log of experiment 2",
    "section": "19.3 Log",
    "text": "19.3 Log\n\n\n\n\n\n\n\n\n\nTimestamp\nTime from start (minutes)\nDuration\nLog\n\n\n\n\n11:00:02 AM\n0\n0\nB1: Start data collection (ipfixprobe and logger)\n\n\n11:00:59 AM\n1\n1\nStarted traffic2.py\n\n\n11:17:15 AM\n17\n16\nStop data collection, save result to v7_1.csv\n\n\n11:36:11 AM\n36\n19\nStop traffic2.py, save v7_exp_1.log\n\n\n11:44:50 AM\n45\n9\nB2: Start data collection again (ipfixprobe and logger)\n\n\n11:46:00 AM\n46\n1\nStarted traffic2.py\n\n\n11:56:59 AM\n57\n11\nStarted infect.sh\n\n\n12:14:18 PM\n74\n17\nStop data collection, save result to v7_2.csv\n\n\n12:14:31 PM\n74\n0\nStop traffic2.py, save v7_exp_2.log"
  },
  {
    "objectID": "flubot-detection-2-overview.html#artifacts",
    "href": "flubot-detection-2-overview.html#artifacts",
    "title": "19  Design and log of experiment 2",
    "section": "19.4 Artifacts",
    "text": "19.4 Artifacts\n\nB1 part of traffic saved into v7_1.csv\nB2 part of traffic saved into v7_2.csv\ntraffic2.py log is saved in v7_exp_1.log and v7_exp_2.log (before and after infection)\nHigh-level log with timestamps of experiment is saved in v7_log.csv"
  },
  {
    "objectID": "notebooks/analyze-collected-dataset-2.html#real-world-scenario-experiment-with-flubot-infected-machine",
    "href": "notebooks/analyze-collected-dataset-2.html#real-world-scenario-experiment-with-flubot-infected-machine",
    "title": "20  Analyze generated dataset",
    "section": "20.1 Real-world scenario experiment with FluBot infected machine",
    "text": "20.1 Real-world scenario experiment with FluBot infected machine\nTraffic stored in v9_1.csv (B1) and v9_2.csv (B2) (Figure 3.1).\nLet’s convert it to feather for faster processing, and get a bird view on the data.\n\ndf = pd.read_csv(basepath + \"datasets/flubot20230323/data/v9_1.csv\")\ndf.to_feather(\"cached/v9_1.ft\")\ndf = pd.read_csv(basepath + \"datasets/flubot20230323/data/v9_2.csv\")\ndf.to_feather(\"cached/v9_2.ft\")\n\n\nimport dateutil.parser\n\ndf1 = pd.read_feather(\"cached/v9_1.ft\")\ndf1[\"time TIME_FIRST\"] = df1[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n\n\nlen(df1)\n\n12167\n\n\n\ndf2 = pd.read_feather(\"cached/v9_2.ft\")\ndf2[\"time TIME_FIRST\"] = df2[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n\n\nlen(df2)\n\n158678\n\n\n\ndf.info()\n\n&lt;class 'modin.pandas.dataframe.DataFrame'&gt;\nRangeIndex: 158678 entries, 0 to 158677\nData columns (total 29 columns):\n #   Column                    Non-Null Count   Dtype \n---  ------------------------  ---------------  ----- \n 0   ipaddr DST_IP             158678 non-null  object\n 1   ipaddr SRC_IP             158678 non-null  object\n 2   uint64 BYTES              158678 non-null  int64\n 3   uint64 BYTES_REV          158678 non-null  int64\n 4   uint64 LINK_BIT_FIELD     158678 non-null  int64\n 5   time TIME_FIRST           158678 non-null  object\n 6   time TIME_LAST            158678 non-null  object\n 7   macaddr DST_MAC           158678 non-null  object\n 8   macaddr SRC_MAC           158678 non-null  object\n 9   uint32 PACKETS            158678 non-null  int64\n 10  uint32 PACKETS_REV        158678 non-null  int64\n 11  uint16 DST_PORT           158678 non-null  int64\n 12  uint16 SRC_PORT           158678 non-null  int64\n 13  uint16 TLS_VERSION        158678 non-null  int64\n 14  uint8 DIR_BIT_FIELD       158678 non-null  int64\n 15  uint8 PROTOCOL            158678 non-null  int64\n 16  uint8 TCP_FLAGS           158678 non-null  int64\n 17  uint8 TCP_FLAGS_REV       158678 non-null  int64\n 18  int8* PPI_PKT_DIRECTIONS  158678 non-null  object\n 19  uint8* PPI_PKT_FLAGS      158678 non-null  object\n 20  string TLS_ALPN           83494 non-null   object\n 21  bytes TLS_JA3             136785 non-null  object\n 22  string TLS_SNI            136783 non-null  object\n 23  uint16* PPI_PKT_LENGTHS   158678 non-null  object\n 24  uint32* D_PHISTS_IPT      158678 non-null  object\n 25  uint32* D_PHISTS_SIZES    158678 non-null  object\n 26  uint32* S_PHISTS_IPT      158678 non-null  object\n 27  uint32* S_PHISTS_SIZES    158678 non-null  object\n 28  time* PPI_PKT_TIMES       158678 non-null  object\ndtypes: object(17), int64(12)\nmemory usage: 35.1 MB\n\n\n\nmax_ts_b1 = df1[\"time TIME_FIRST\"].max() - pd.Timedelta(minutes=5)\n\n\n# skip last 5 mins of traffic where we have fluctiations because of stopped traffic2.py script\nlocal_df1 = df1[df1[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\nlocal_df1 = local_df1[local_df1[\"time TIME_FIRST\"] &lt; max_ts_b1]\n\nlocal_df2 = df2[df2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\nlen(local_df1), len(local_df2)\n\n(8870, 149890)\n\n\n\nlocal_ips2 = local_df2[\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips2\n\n\nlocal_ips1 = local_df1[\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips = list(set(local_ips1 + local_ips2))\nlocal_ips\n\n['192.168.2.149', '192.168.2.42', '192.168.2.217', '192.168.1.1']\n\n\n\nmin_ts = local_df1[\"time TIME_FIRST\"].min()\nmin_ts_b2 = local_df2[\"time TIME_FIRST\"].min()\nmin_ts, min_ts_b2\n\n(Timestamp('2023-03-27 06:34:34.435834'),\n Timestamp('2023-03-27 08:11:28.539603'))"
  },
  {
    "objectID": "notebooks/analyze-collected-dataset-2.html#b1-period-vs-b2-period",
    "href": "notebooks/analyze-collected-dataset-2.html#b1-period-vs-b2-period",
    "title": "20  Analyze generated dataset",
    "section": "20.2 B1 period vs B2 period",
    "text": "20.2 B1 period vs B2 period\n\ninfection_ts = min_ts + pd.Timedelta(minutes=160)\ninfection_ts\n\nTimestamp('2023-03-27 09:14:34.435834')\n\n\n\nlocal_df2_a1 = local_df2[local_df2[\"time TIME_FIRST\"] &lt; infection_ts]\nlocal_df2_a2 = local_df2[local_df2[\"time TIME_FIRST\"] &gt;= infection_ts]\n\n\nfrom matplotlib import pyplot as plt\n\nd = {\n    'B1': local_df1[\"ipaddr SRC_IP\"].value_counts().to_dict(),\n    'B2 (A1)': local_df2_a1[\"ipaddr SRC_IP\"].value_counts().to_dict(),\n    'B2 (A2)': local_df2_a2[\"ipaddr SRC_IP\"].value_counts().to_dict(),\n}\n\npd.DataFrame(d).plot(kind='bar', log=True)\nplt.show()"
  },
  {
    "objectID": "notebooks/analyze-collected-dataset-2.html#plaintext-dns-vs-whole-https-traffic",
    "href": "notebooks/analyze-collected-dataset-2.html#plaintext-dns-vs-whole-https-traffic",
    "title": "20  Analyze generated dataset",
    "section": "20.3 Plaintext DNS vs whole HTTPS traffic",
    "text": "20.3 Plaintext DNS vs whole HTTPS traffic\n\nfrom matplotlib import pyplot as plt\n\nfig, axs = plt.subplots(ncols=3, figsize=(12, 4))\n\nmax_val = 0\nfor ax, df, title in zip(axs, [local_df1, local_df2_a1, local_df2_a2], [\"B1\", \"B2 (A1)\", \"B2 (A2)\"]):\n    d = {\n        '443': df[df[\"uint16 DST_PORT\"] == 443][\"ipaddr SRC_IP\"].value_counts().to_dict(),\n        '53': df[df[\"uint16 DST_PORT\"] == 53][\"ipaddr SRC_IP\"].value_counts().to_dict(),\n    }\n    max_val = max(max_val, *[v2 for v in d.values() for v2 in v.values()])\n    pd.DataFrame(d).plot(kind='bar', log=True, ax=ax, title=title)\n\nfor ax in axs:\n    ax.set_ylim(ymin=0, ymax=max_val)\n    \nplt.show()"
  },
  {
    "objectID": "notebooks/analyze-collected-dataset-2.html#timeline-of-the-traffic",
    "href": "notebooks/analyze-collected-dataset-2.html#timeline-of-the-traffic",
    "title": "20  Analyze generated dataset",
    "section": "20.4 Timeline of the traffic",
    "text": "20.4 Timeline of the traffic\n\nf = '2min'\n\ndef plot_packets_by_port(local_df, host, dst_port):\n    a1_t = local_df[local_df[\"ipaddr SRC_IP\"] == host].set_index([\"time TIME_FIRST\"])\n\n    a1_t53 = a1_t[a1_t[\"uint16 DST_PORT\"] == dst_port][[\"uint32 PACKETS\", \"uint32 PACKETS_REV\"]]\n    a1_t53[\"PACKETS\"] = a1_t53[\"uint32 PACKETS\"] + a1_t53[\"uint32 PACKETS_REV\"]\n    df_resample = a1_t53.groupby(pd.Grouper(freq=f, origin=min_ts)).sum()\n    return df_resample\n\n\nlocal_df = pd.concat([local_df1, local_df2]).reset_index(drop=True)\n\n\ndf_plots = {h: plot_packets_by_port(local_df, h, 53) for h in local_ips}\n\n\npd.DataFrame({\n    h: df[\"PACKETS\"].to_dict()\n    for h, df in df_plots.items()\n    if len(df) &gt; 0\n}).plot()\n\nplt.axvline(x=min_ts_b2, label=\"B2 period start\", color=\"green\")\nplt.axvline(x=infection_ts, label=\"Infection\", color=\"red\")\n\nplt.title(\"Plaintext DNS packets\")\nplt.legend()\nplt.plot()\n\n[]\n\n\n\n\n\n\ndf_plots = {h: plot_packets_by_port(local_df, h, 443) for h in local_ips}\n\n\npd.DataFrame({\n    h: df[\"PACKETS\"].to_dict()\n    for h, df in df_plots.items()\n    if len(df) &gt; 0\n}).plot(logy=True)\n\nplt.axvline(x=min_ts_b2, label=\"B2 period start\", color=\"green\")\nplt.axvline(x=infection_ts, label=\"Infection\", color=\"red\")\n\nplt.title(\"HTTPS packets\")\nplt.legend()\nplt.plot()\n\n[]"
  },
  {
    "objectID": "notebooks/flubot-detection-second.html",
    "href": "notebooks/flubot-detection-second.html",
    "title": "21  Detect malware",
    "section": "",
    "text": "The model deployment is done in two stages: I. tune the model, II. turn on the malware detection (on the data collected in B1 and B2 periods respectively, described in the timeline Figure 3.1)\n\n\n\nTimeline\n\n\n\n21.0.1 Model selection\nIn experiment I we have seen that the model “RF normed-2-4-0-0” performed better than others. We will use this model in the current experiment.\nModel description\n\nRandom Forest classification model\nNormalization with RobustClassifier is enabled\nModel skips first 2 packets and next 4 packets are weighted for packet sizes statistics calculation\n\n\n\n21.0.2 Stage I: Model tuning\nOn this state we fit the scaler on the data collected in B1 period, to be used later on the second stage\n\nfrom detect_common import *\n\n\np1version = \"v6\"\np2version = \"v7\"\np3version = \"v9\"\np3version_exp1 = \"v5\"\n\n\nmodel = 'RF'\nnormed = True\nskips_and_weights = [2,4,0,0]\n\n\nimport dateutil.parser\n\ndf_b1 = pd.read_feather(\"cached/v9_1.ft\")\n\n\npath_doh_ips_androids = basepath + \"datasets/flubot20230323/data/v5_benign_doh_ips.csv\"\npath_doh_ips_general = \"doh_resolver_ip.csv\"\nfull_list = get_doh_ips(path_doh_ips_androids) + get_doh_ips(path_doh_ips_general)\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b1_p = cp.process(df_b1, f\"cached/v9_1.p3.{p3version}.cp.ft\")\n\n\nfrom joblib import dump, load\n\nname = (\n    ('normed-' if normed else 'unnormed-') + \n    '-'.join(str(p) for p in skips_and_weights)\n)\n\nclf = load(f'models/{model}-mix-1-p-{name}.p1.{p1version}.p2.{p2version}.joblib')\nmodel, name\n\n('RF', 'normed-2-4-0-0')\n\n\n\nresults_df = pd.read_feather(f\"cached/robustness.balanced_mix-results.p3.{p3version_exp1}.ft\")\nresults_df[\"skips_and_weights\"] = results_df[\"skips_and_weights\"].apply(str)\nmodel, normed, skips_and_weights_str, auc, fprs, tprs, thresholds = results_df[\n    (results_df[\"model\"] == model) & (results_df[\"normed\"] == normed) & (results_df[\"skips_and_weights\"] == str(np.array(skips_and_weights)))\n].iloc[0]\n\n\npp_b1 = PreProcessing(normed, *skips_and_weights)\n\ndf_b1_pp = pp_b1.process(\n    df_b1_p,\n    f\"cached/p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=True\n)\n\n\nif not os.path.exists(f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"):\n    pp_b1.store_scaler(\n        f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n    )\n\n\nlocal_ips = df_b1[df_b1[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")][\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips\n\n['192.168.2.149', '192.168.2.217', '192.168.2.42']\n\n\n\ndef predict(pp, clf, threshold, df_pp, df_orig):\n    \"\"\"\n    pp: PreProcessing object\n    clf: classifier\n    threshold: &gt;=threshold for positive (DoH) class\n    df_pp: preprocessed dataframe\n    df_orig: unprocessed dataframe\n    \"\"\"\n    df_pp2 = df_pp.copy()\n\n    df_pp2[\"IsDoHPredicted\"] = (clf.predict_proba(df_pp2[pp.feature_fields])[:,1] &gt;= threshold).astype(bool)\n\n    df_orig2 = df_orig.copy()\n    # fill predicted to the original dataframe\n    df_orig2[\"IsDoHPredicted\"] = df_pp2[\"IsDoHPredicted\"]\n    # non-443 rows will be NaN, replace with False\n    df_orig2[\"IsDoHPredicted\"] = df_orig2[\"IsDoHPredicted\"].fillna(False)\n\n    return df_orig2\n\n\ndef calc(df_n):\n    def rate(df_h):\n        dns_packets = df_h[df_h[\"IsDoHPredicted\"] == True][\"uint32 PACKETS\"].sum()  # outgoing packets\n        non_dns_ips = set(\n            df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr SRC_IP\"].unique()\n        ).union(\n            # src and dst IPs could be stored in a reverse way.\n            # since number of internal IPs is limited, it's safe to account them all\n            set(\n                df_h[df_h[\"IsDoHPredicted\"] == False][\"ipaddr DST_IP\"].unique()\n            )\n        )\n        rate = float(dns_packets) / (1.0 + len(non_dns_ips))\n        return {\n            \"dns_packets\": dns_packets,\n            \"non_dns\": len(non_dns_ips),\n            \"rate\": rate,\n            \"rate_log\": np.log(rate)\n        }\n\n    return pd.DataFrame(\n        { ip: rate(grp) for ip, grp in df_n.groupby([\"ipaddr SRC_IP\"]) }\n    ).T.reset_index()\n\n\n21.0.2.1 Build time-series forecast models for each host\n\nfreq = \"2min\"\n\nbehavior_ips_sens = {}\n\nfor sensitivity in [0.8, 0.9]:  #, 0.95]:\n    print(sensitivity)\n    threshold = [threshold for tpr, threshold in zip(tprs, thresholds) if tpr &gt;= sensitivity][0]\n\n    min_ts = df_b1[\"time TIME_FIRST\"].apply(dateutil.parser.parse).min()\n\n    df_b1_pp_df = df_b1_pp[0]\n    behavior_ips = {}\n    for ip in local_ips:\n        print(ip)\n        df_b1_ip_pp = df_b1_pp_df[df_b1_pp_df[\"ipaddr SRC_IP\"] == ip]\n        if len(df_b1_ip_pp) == 0:\n            continue\n        #df_b1_ip_pp[\"time TIME_FIRST\"] = df_b1_ip_pp[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n        # df_b1_ip_pp = df_b1_ip_pp.set_index(\"time TIME_FIRST\")\n        df_b1_ip_pp_pred = predict(\n            pp_b1, clf, threshold, df_b1_ip_pp, df_b1[df_b1[\"ipaddr SRC_IP\"] == ip]\n        )\n\n        df_b1_ip_pp_pred[\"time TIME_FIRST\"] = df_b1_ip_pp_pred[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n        # ignore last 5 mins\n        df_b1_ip_pp_pred_max = df_b1_ip_pp_pred[\"time TIME_FIRST\"].max()  # - pd.Timedelta(minutes=5)\n        df_b1_ip_pp_pred = df_b1_ip_pp_pred[df_b1_ip_pp_pred[\"time TIME_FIRST\"] &lt; df_b1_ip_pp_pred_max]\n        \n        df_b1_ip_pp_pred = df_b1_ip_pp_pred.set_index([\"time TIME_FIRST\"])\n\n        df_resample = df_b1_ip_pp_pred.groupby(pd.Grouper(freq=freq, origin=min_ts)).first()\n        times = df_resample.index.tolist()\n        ranges = list(zip(times, times[1:]))\n\n        df_calcs = []\n        for ran in ranges:\n            df_ran = df_b1_ip_pp_pred[(df_b1_ip_pp_pred.index &gt; ran[0]) & (df_b1_ip_pp_pred.index &lt; ran[1])]\n            df_calc = calc(df_ran)\n            df_calc[\"ts\"] = ran[0]\n            df_calcs.append(df_calc)\n\n        behavior_ips[ip] = df_calcs\n\n    behavior_ips_sens[sensitivity] = behavior_ips\n\n0.8\n192.168.2.149\n\n\n192.168.2.217\n\n\n192.168.2.42\n0.9\n192.168.2.149\n\n\n192.168.2.217\n\n\n192.168.2.42\n\n\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nforecast_models = {}  # (sensitivity, ip addr) =&gt; time-series forecasting model\nwindow_size = 10\n\ndef moving_average(x, w):\n    # https://stackoverflow.com/questions/14313510/how-to-calculate-rolling-moving-average-using-python-numpy-scipy\n    return np.convolve(x, np.ones(w), 'valid') / w\n\n\nfor sens, behavior_ips in behavior_ips_sens.items():\n    for ip, data_ips in behavior_ips.items():\n        rate_logs = np.array([\n            item[\"rate_log\"][0] if len(item) else 0.0\n            for item in data_ips\n        ])\n        forecast_model = ARIMA(\n            # rate_logs\n            moving_average(rate_logs, window_size)\n        ).fit()\n        forecast_models[(sens, ip)] = forecast_model\n\n\n\n\n21.0.3 Stage II: Malware detection\nOn this state we turn on the malware detection, provisioning the model with the scaler trained before. For malware detection we use the data trained in B2 period.\n\ndf_b2 = pd.read_feather(\"cached/v9_2.ft\")\n\n\ncp = CacheableProcessing(100, 100, full_list)\ndf_b2_p = cp.process(df_b2, f\"cached/v9_2.p3.{p3version}.cp.ft\")\n\n\npp_b2 = PreProcessing(normed, *skips_and_weights)\npp_b2.load_scaler(\n    f\"cached/p3.{p3version}.pp.{model}-{name}.scaler\"\n)\nassert pp_b2._scaler\ndf_b2_pp, df_b2_pp_features, df_b2_pp_labels = pp_b2.process(\n    df_b2_p,\n    f\"cached/b2.p3.{p3version}.pp.{model}-{name}.saved\", \n    fit_new_scaler=False  # use scaler trained in B1\n)\n\n\nmin_ts = df_b2[\"time TIME_FIRST\"].apply(dateutil.parser.parse).min()\ninfection_ts = min_ts + pd.Timedelta(minutes=62) - pd.Timedelta(freq) * window_size\nmin_ts, infection_ts\n\n(Timestamp('2023-03-27 08:11:28.503294'),\n Timestamp('2023-03-27 08:53:28.503294'))\n\n\n\ndata = {}\nfor sensitivity in [0.8, 0.9]:  #, 0.95]:\n    threshold = [threshold for tpr, threshold in zip(tprs, thresholds) if tpr &gt;= sensitivity][0]\n    df_pred = predict(pp_b2, clf, threshold, df_b2_pp, df_b2)\n\n    data[sensitivity] = {\n        'df': df_pred,  # dataframe with predicted values,\n        'threshold': threshold,\n    }\n\n\n21.0.3.1 Calculate ratio on the whole dataset: benign period (A1) and infected (A2)\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")]\n\n    local_df[\"time TIME_FIRST\"] = local_df[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n\n    local_df_a1 = local_df[local_df[\"time TIME_FIRST\"] &lt; infection_ts]\n    local_df_a2 = local_df[local_df[\"time TIME_FIRST\"] &gt;= infection_ts]\n\n    print(\"sensitivity:\", sensitivity, \"A1\")\n    display(calc(local_df_a1))\n    print(\"sensitivity:\", sensitivity, \"A2\")\n    display(calc(local_df_a2))\n\n\n0.8\n\n\nsensitivity: 0.8 A1\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.2.149\n79451.0\n2058.0\n38.587178\n3.652920\n\n\n1\n192.168.2.217\n64126.0\n1748.0\n36.664380\n3.601806\n\n\n2\n192.168.2.42\n0.0\n3.0\n0.000000\n-inf\n\n\n\n\n\n\n\nsensitivity: 0.8 A2\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n53922.0\n1524.0\n35.358689\n3.565544\n\n\n2\n192.168.2.217\n146060.0\n1816.0\n80.385250\n4.386831\n\n\n3\n192.168.2.42\n0.0\n4.0\n0.000000\n-inf\n\n\n\n\n\n\n\n0.9\n\n\nsensitivity: 0.9 A1\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.2.149\n92981.0\n1961.0\n47.390928\n3.858431\n\n\n1\n192.168.2.217\n76559.0\n1692.0\n45.220910\n3.811560\n\n\n2\n192.168.2.42\n0.0\n3.0\n0.000000\n-inf\n\n\n\n\n\n\n\nsensitivity: 0.9 A2\n\n\n\n\n\n\n\n\n\nindex\ndns_packets\nnon_dns\nrate\nrate_log\n\n\n\n\n0\n192.168.1.1\n0.0\n2.0\n0.000000\n-inf\n\n\n1\n192.168.2.149\n65813.0\n1457.0\n45.139232\n3.809752\n\n\n2\n192.168.2.217\n418387.0\n1730.0\n241.702484\n5.487708\n\n\n3\n192.168.2.42\n0.0\n4.0\n0.000000\n-inf\n\n\n\n\n\n\n\n\n\n21.0.3.2 Split into time windows\n\nlocal_ips = df_b2[df_b2[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")][\"ipaddr SRC_IP\"].unique().tolist()\nlocal_ips\n\n['192.168.2.149', '192.168.2.217', '192.168.2.42', '192.168.1.1']\n\n\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n\n    df_pred = value[\"df\"]\n    local_df = df_pred[df_pred[\"ipaddr SRC_IP\"].str.startswith(\"192.168\")].copy()\n    local_df[\"time TIME_FIRST\"] = local_df[\"time TIME_FIRST\"].apply(dateutil.parser.parse)\n\n    calc_ips = {}\n\n    a1_t = local_df.set_index([\"time TIME_FIRST\"])\n\n    df_resample = a1_t.groupby(pd.Grouper(freq=freq, origin=min_ts)).first()\n    times = df_resample.index.tolist()\n    ranges = list(zip(times, times[1:]))\n\n    df_calcs = []\n    for ran in ranges:\n        df_ran = a1_t[(a1_t.index &gt; ran[0]) & (a1_t.index &lt; ran[1])]\n        df_calc = calc(df_ran)\n        df_calc[\"ts\"] = ran[0]\n        df_calcs.append(df_calc)\n\n    value[\"df_calc\"] = pd.concat(df_calcs).reset_index(drop=True)\n\n0.8\n\n\n0.9\n\n\n\nclean = \"192.168.2.149\"\ninfected = \"192.168.2.217\"\n\nWe are using use ARIMA model to calculate confidence interval upper limit for outlier (malware) detection\n\n\nimport matplotlib.pyplot as plt\n\nfor sensitivity, value in data.items():\n    print(sensitivity)\n    fig, axs = plt.subplots(ncols=1, figsize=(12, 4))\n    df_calcs = value[\"df_calc\"]\n\n    df_calcs_clean = df_calcs[df_calcs[\"index\"] == clean].set_index(\"ts\")\n    df_calcs_infected = df_calcs[df_calcs[\"index\"] == infected].set_index(\"ts\")\n    for host, df, color, hatch in zip(\n        [infected, clean], \n        [df_calcs_infected, df_calcs_clean], \n        ['blue', 'green'],\n        ['\\\\', '/'],\n    ):\n        df_avgd = df.copy()\n        df_avgd_array = moving_average(df[\"rate_log\"], window_size)\n        rate_log_ser = pd.Series(df_avgd_array, index=df_avgd[\"rate_log\"].index[:len(df_avgd_array)])\n\n        rate_log_ser.plot(\n            label=f'rate_log of {host}, mean rolling window={window_size}',\n            ax=axs, color=color\n        )\n        if (sensitivity, host) in forecast_models:\n            forecast_model = forecast_models[(sensitivity, host)]\n            forecast = forecast_model.get_forecast(len(rate_log_ser))\n\n            yhat_conf_int = forecast.conf_int(alpha=0.05)\n            under_line = [v[0] for v in yhat_conf_int]\n            over_line = [v[1] for v in yhat_conf_int]\n            plt.fill_between(\n                rate_log_ser.index, under_line,\n                over_line, color=color, alpha=.1,\n                hatch=hatch, edgecolor='black'\n            )\n            plt.fill(\n                np.NaN, np.NaN, color=color, alpha=0.5,\n                label=f'{host} confidence interval', hatch=hatch\n            )\n\n    plt.axvline(\n        x=infection_ts, label=\"Infection (B2 period start)\", color=\"red\", linestyle='--', alpha=0.5\n    )\n\n    # plt.ylim(ymin=0)\n    plt.title(f\"Sensitivity: {sensitivity}, threshold: {value['threshold']}\")\n    plt.legend()\n    plt.show()\n\n0.8\n\n\n\n\n\n(a) sensitivity 0.8\n\n\n\n\n0.9\n\n\n\n\n\n(b) sensitivity 0.9\n\n\n\nFigure 21.1: \\(\\rho(a)\\) plot for clean and infected instances (B2 period)"
  },
  {
    "objectID": "summary.html#dns-over-https-detection-model-training-results",
    "href": "summary.html#dns-over-https-detection-model-training-results",
    "title": "22  Results",
    "section": "22.1 DNS-over-HTTPS detection model training results",
    "text": "22.1 DNS-over-HTTPS detection model training results\nWe have built a number of models based on different algorithms and hyperparameters. Models were evaluated on the “robustness”: ability to detect DoH not only in the same network where training data was recorded, but on other networks too. The test dataset containing 50% from the test sample of training data, and 50% from the recorded traffic from sandbox network. ROCs of these models are in Figure 22.1.\n\n\n\n\n\n\n\n(a) Logistic Regression\n\n\n\n\n\n\n\n(b) Hist Gradient Boosting\n\n\n\n\n\n\n\n(c) Random Forest\n\n\n\nFigure 22.1: ROC and their AUC for test dataset\n\n\nSource: choose-model.ipynb\nTop 3 models with normalization based on their AUC:\n\n\n\n\n\n\n\n\n\n\nmodel\nnormed\nskips_and_weights\ntest_data_auc\nfpr\ntpr\nthresholds\n\n\n\n\n23\nRF\nTrue\n[2, 4, 0, 0]\n0.9355\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00010794...\n[0.0, 0.00010486577181208053, 0.00031459731543...\n[1.98, 0.98, 0.97, 0.96, 0.95, 0.94, 0.92, 0.9...\n\n\n44\nRF\nTrue\n[2, 4, 2, 4]\n0.9324\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.000107944732297063...\n[0.0, 0.00041946308724832214, 0.00094379194630...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n5\nRF\nTrue\n[1, 0, 0, 0]\n0.9278\n[0.0, 0.0, 0.0, 0.0, 0.0001079447322970639, 0....\n[0.0, 0.00010486577181208053, 0.00020973154362...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.95, 0.93, 0.92,...\n\n\n\n\n\n\n\nSource: choose-model.ipynb\nTop 3 models without normalization based on their AUC:\n\n\n\n\n\n\n\n\n\n\nmodel\nnormed\nskips_and_weights\ntest_data_auc\nfpr\ntpr\nthresholds\n\n\n\n\n53\nRF\nFalse\n[2, 0, 0, 0]\n0.9780\n[0.0, 0.0, 0.0, 0.0001079447322970639, 0.00010...\n[0.0, 0.3485738255033557, 0.3976510067114094, ...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n68\nRF\nFalse\n[2, 4, 0, 0]\n0.9761\n[0.0, 0.0, 0.0, 0.0001079447322970639, 0.00021...\n[0.0, 0.34133808724832215, 0.3921979865771812,...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n86\nRF\nFalse\n[1, 4, 1, 4]\n0.9753\n[0.0, 0.0, 0.0, 0.0002158894645941278, 0.00021...\n[0.0, 0.3493078859060403, 0.39890939597315433,...\n[2.0, 1.0, 0.99, 0.98, 0.97, 0.96, 0.95, 0.94,...\n\n\n\n\n\n\n\nSource: choose-model.ipynb"
  },
  {
    "objectID": "summary.html#flubot-detection-clean-room-experiment-1-results",
    "href": "summary.html#flubot-detection-clean-room-experiment-1-results",
    "title": "22  Results",
    "section": "22.2 Flubot detection “Clean room” experiment 1 results",
    "text": "22.2 Flubot detection “Clean room” experiment 1 results\nFor this experiment we took 6 models mentioned above, and two models (normed and unnormed) with hyperparameters equal to [0,0,0,0], for the baseline comparison.\nInterestingly, normalized model performed better than the unnormalized, which aligns with the original hypothesis but doesn’t align with the training test model comparison.\nIn the result, normed models with non-0 hyperparameters perform better or same as the baseline 0-0-0-0 models.\nUnnormed models with non-0 hyperparameters perform worse than the baseline 0-0-0-0 models.\n\n22.2.1 Baseline models\nRF-unnormed-0-0-0-0:\n\n\n\n\n0.8\n\n\n\n\n\n(a) sensitivity 0.8\n\n\n\n\n0.9\n\n\n\n\n\n(b) sensitivity 0.9\n\n\n\nFigure 22.2: \\(\\rho(a)\\) plot for all instances (B2 period)\n\n\nSource: gen-flubot-detection-RF-unnormed-0-0-0-0.ipynb\nRF-normed-0-0-0-0:\n\n\n\n\n0.8\n\n\n\n\n\n(a) sensitivity 0.8\n\n\n\n\n0.9\n\n\n\n\n\n(b) sensitivity 0.9\n\n\n\nFigure 22.3: \\(\\rho(a)\\) plot for all instances (B2 period)\n\n\nSource: gen-flubot-detection-RF-normed-0-0-0-0.ipynb\n\n\n22.2.2 Best unnormed and normed models\nRF-unnormed-2-0-0-0:\n\n\n\n\n0.8\n\n\n\n\n\n(a) sensitivity 0.8\n\n\n\n\n0.9\n\n\n\n\n\n(b) sensitivity 0.9\n\n\n\nFigure 22.4: \\(\\rho(a)\\) plot for all instances (B2 period)\n\n\nSource: gen-flubot-detection-RF-unnormed-2-0-0-0.ipynb\nRF-normed-2-4-0-0:\n\n\n\n\n0.8\n\n\n\n\n\n(a) sensitivity 0.8\n\n\n\n\n0.9\n\n\n\n\n\n(b) sensitivity 0.9\n\n\n\nFigure 22.5: \\(\\rho(a)\\) plot for all instances (B2 period)\n\n\nSource: gen-flubot-detection-RF-normed-2-4-0-0.ipynb"
  },
  {
    "objectID": "summary.html#flubot-detection-real-world-experiment-2-results",
    "href": "summary.html#flubot-detection-real-world-experiment-2-results",
    "title": "22  Results",
    "section": "22.3 Flubot detection “Real-world” experiment 2 results",
    "text": "22.3 Flubot detection “Real-world” experiment 2 results\nIn the last experiment we took the model which performed the best on the first experiment (RF-normed-2-4-0-0) and used to detect the machine browsing benign websites which is exposed to the FluBot malware at some point.\nARIMA time-series model was used to learn the hosts normal behavior and detect outliers (FluBot infection):\n\n\n\n\n0.8\n\n\n\n\n\n(a) sensitivity 0.8\n\n\n\n\n0.9\n\n\n\n\n\n(b) sensitivity 0.9\n\n\n\nFigure 22.6: \\(\\rho(a)\\) plot for clean and infected instances (B2 period)\n\n\nSource: flubot-detection-second.ipynb"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Grill, Martin, Ivan Nikolaev, Veronica Valeros, and Martin Rehak. 2015.\n“Detecting DGA Malware Using\nNetFlow.” In 2015 IFIP/IEEE\nInternational Symposium on Integrated Network\nManagement (IM), 1304–9. https://doi.org/10.1109/INM.2015.7140486.\n\n\nJeřábek, Kamil, Karel Hynek, Tomáš Čejka, and Ondřej Ryšavý. 2022.\n“Collection of Datasets with DNS over\nHTTPS Traffic.” Data in Brief 42 (June):\n108310. https://doi.org/10.1016/j.dib.2022.108310."
  },
  {
    "objectID": "appendices.html",
    "href": "appendices.html",
    "title": "Appendix A — Library",
    "section": "",
    "text": "notebooks/detect_common.py:\nimport os\nimport modin.pandas as pd\nimport numpy as np\nimport dateutil\nfrom pathlib import Path\n\nfrom sklearn.preprocessing import PolynomialFeatures, normalize, RobustScaler\nfrom joblib import dump, load\n\n\nHYPERPARAMS = [\n    [0, 0, 0, 0],\n\n    [1, 0, 0, 0],\n    [2, 0, 0, 0],\n    [4, 0, 0, 0],\n    [6, 0, 0, 0],\n    [0, 4, 0, 0],\n    [1, 4, 0, 0], \n    [2, 4, 0, 0],\n\n    [1, 0, 1, 0],\n    [2, 0, 2, 0],\n    [4, 0, 4, 0],\n    [6, 0, 6, 0],\n    [0, 4, 0, 4],\n    [1, 4, 1, 4],\n    [2, 4, 2, 4],\n]\n\n\ndef enum_csv(path):\n    csv_folder = Path(path)\n    for p in csv_folder.glob('**/*.csv'):\n        yield p\n\n\ndef ipfix_list_to_array(series, convert=int, limit=None):\n    return series.map(\n        lambda val: np.array([\n            convert(x) \n            for x in val.strip(\"[]\").split(\"|\") if x != ''\n        ])[:limit]\n    )\n\n\ndef direction_sizes_filter(direction):\n    return lambda x: np.array([\n        sz\n        for sz, dir_, in zip(x[0], x[1])\n        if dir_ == direction\n    ])\n\n\ndef millis_diffs(arr, pkts_limit):  # Zhan 2022 discards total bytes but uses Time intervals\n    times = ipfix_list_to_array(arr, dateutil.parser.parse, limit=pkts_limit+1)\n    return times.apply(lambda arr: np.array([(b - a).total_seconds() * 1000 for a, b in zip(arr, arr[1:])]))\n\n\ndef get_doh_ips(path_doh_ips):\n    doh_ips_df = pd.read_csv(path_doh_ips)\n    return doh_ips_df.iloc[:,0].to_list()\n\n\ndef stat_funcs(skip, weighted_num):\n    def weights(a):\n        n = len(a)\n        zeros = min(skip, n)\n        weights = min(n - zeros, weighted_num + 1)\n        ones = max(n - zeros - weights, 0)\n        return np.concatenate([\n            np.zeros(zeros),\n            np.linspace(0.1, 1, weights),\n            np.ones(ones),\n        ])\n\n    def var(a__mean):\n        a, average = np.array(a__mean[0]), a__mean[1]\n        w = weights(a)\n        return np.average((a-average)**2, weights=w) if np.sum(w) &gt; 0 else 0.0\n\n    def mean(a):\n        w = weights(a)\n        return np.average(a, weights=w) if np.sum(w) &gt; 0 else 0.0\n    return mean, var\n\n\ndef fields_stats(dfx, fields, skip=0, weighted_num=0):\n    mean, var = stat_funcs(skip, weighted_num)\n\n    for field in fields:\n        dfx[f\"{field}_mean\"] = dfx[field].apply(mean)\n        dfx[f\"{field}_var\"] = dfx[[field, f\"{field}_mean\"]].apply(var, axis=1)\n        dfx[f\"{field}_stddev\"] = np.sqrt(dfx[f\"{field}_var\"])\n        \n    return dfx\n\n\ndef filter_significant_flows(dfx):\n    return dfx[\n        (dfx[\"uint32 PACKETS\"] &gt; 2) &\n        (dfx[\"uint32 PACKETS_REV\"] &gt; 2) &\n        (dfx[\"uint16* PPI_PKT_LENGTHS\"].str.len() &gt; 2) &\n        (dfx[\"uint16* PPI_PKT_LENGTHS_1\"].str.len() &gt; 2) &\n        (dfx[\"uint16* PPI_PKT_LENGTHS_-1\"].str.len() &gt; 2)\n    ]\n\n\nclass CacheableProcessing:\n    def __init__(self, len_pkts_limit, times_pkts_limit, doh_ips=None):\n        self._len_pkts_limit = len_pkts_limit\n        self._times_pkts_limit = times_pkts_limit\n        self._doh_ips = doh_ips\n\n        self._is_labeled = False\n        self._is_filtered = False\n        self._is_processed = False\n    \n    def _reversed_rows(self, dfx):\n        reversed_rows = dfx[\"uint16 SRC_PORT\"] &lt; 1024\n        dfx = dfx[~reversed_rows]  # ignore reversed rows\n        return dfx\n    \n    def _filter_out_icmp(self, dfx):\n        return dfx[(dfx[\"uint8 PROTOCOL\"] != 58) & (dfx[\"uint8 PROTOCOL\"] != 1)]  # no ICMPv6 and ICMP\n\n    def _filter_443(self, dfx):\n        dfx = dfx[dfx[\"uint16 DST_PORT\"] == 443]  # filter HTTPS\n        self._is_filtered = True\n        return dfx\n\n    def clean(self, df):\n        dfx = df.copy()\n        dfx = self._filter_out_icmp(dfx)\n        dfx = self._reversed_rows(dfx)\n        return dfx\n    \n    def _pkt_lengths(self, dfx, pkts_limit):\n        dfx[\"uint16* PPI_PKT_LENGTHS\"] = ipfix_list_to_array(dfx[\"uint16* PPI_PKT_LENGTHS\"], limit=pkts_limit)\n        dfx[\"int8* PPI_PKT_DIRECTIONS\"] = ipfix_list_to_array(dfx[\"int8* PPI_PKT_DIRECTIONS\"], limit=pkts_limit)\n\n        dfx[\"uint16* PPI_PKT_LENGTHS_1\"] = dfx[[\"uint16* PPI_PKT_LENGTHS\", \"int8* PPI_PKT_DIRECTIONS\"]].apply(\n            direction_sizes_filter(1), axis=1\n        )\n        dfx[\"uint16* PPI_PKT_LENGTHS_-1\"] = dfx[[\"uint16* PPI_PKT_LENGTHS\", \"int8* PPI_PKT_DIRECTIONS\"]].apply(\n            direction_sizes_filter(-1), axis=1\n        )\n        return dfx\n\n    def _pkt_times(self, dfx, pkts_limit):\n        dfx[\"PPI_PKT_INTERVALS\"] = millis_diffs(dfx[\"time* PPI_PKT_TIMES\"], pkts_limit)\n\n        dfx[\"PPI_PKT_INTERVALS_1\"] = dfx[[\"PPI_PKT_INTERVALS\", \"int8* PPI_PKT_DIRECTIONS\"]].apply(\n            direction_sizes_filter(1), axis=1\n        )\n        dfx[\"PPI_PKT_INTERVALS-1\"] = dfx[[\"PPI_PKT_INTERVALS\", \"int8* PPI_PKT_DIRECTIONS\"]].apply(\n            direction_sizes_filter(-1), axis=1\n        )\n        return dfx\n\n    def _label_dataset(self, dfx):\n        dfx[\"IsDoH\"] = (dfx[\"ipaddr DST_IP\"].isin(self._doh_ips) & (dfx[\"uint16 DST_PORT\"] == 443))\n        self._is_labeled = True\n        return dfx\n\n    def load(self, stored_df_file):\n        if stored_df_file and os.path.exists(stored_df_file):\n            self._df = pd.read_json(stored_df_file).sort_index()\n            self._is_processed = True\n            return True\n        return False\n\n    def store(self, stored_df_file):\n        self._df.to_json(stored_df_file)\n\n    def process(self, df, stored_df_file):\n        if not self.load(stored_df_file):\n            dfx = df.copy()\n            dfx = self._reversed_rows(dfx)\n            dfx = self._filter_443(dfx)\n            dfx = self._pkt_lengths(dfx, self._len_pkts_limit)\n            dfx = self._pkt_times(dfx, self._times_pkts_limit)\n\n            if self._doh_ips is not None:\n                dfx = self._label_dataset(dfx)\n\n            # dfx = dfx.reset_index(drop=True)\n            self._df = dfx\n            self._is_processed = True\n            self.store(stored_df_file)\n        return self._df\n\n\nclass PreProcessing:\n    def __init__(self, norm=True, l_skip=0, l_weights=0, t_skip=0, t_weights=0):\n        self._df = None\n        self._df_f = None\n        self._df_l = None\n        self._is_processed = False\n\n        self._do_norm = norm\n        self._scaler = None\n\n        self._l_skip = l_skip\n        self._l_weights = l_weights\n        self._t_skip = t_skip\n        self._t_weights = t_weights\n\n    def _feature_fields(self):\n        base_feature_fields = [\n            \"uint16* PPI_PKT_LENGTHS\",\n            \"uint16* PPI_PKT_LENGTHS_1\",\n            \"uint16* PPI_PKT_LENGTHS_-1\",\n        ]\n        base_feature_fields += [  # interval features\n            \"PPI_PKT_INTERVALS\",\n            \"PPI_PKT_INTERVALS_1\",\n            \"PPI_PKT_INTERVALS-1\",\n        ]\n\n        base_feature_fields_stats = [f\"{field}{suffix}\" for field in base_feature_fields for suffix in [\"_stddev\", \"_mean\", \"_var\"]]\n        add_feature_fields = []\n\n        feature_fields = base_feature_fields_stats + add_feature_fields\n        return feature_fields\n\n    @property\n    def feature_fields(self):\n        return self._feature_fields()\n\n    def load(self, stored_df_file):\n        if stored_df_file and os.path.exists(stored_df_file):\n            self._df = pd.read_json(stored_df_file).sort_index()\n            self._df_f = pd.read_json(stored_df_file + '.features').sort_index()\n            self._df_l = pd.read_json(stored_df_file + '.labels').sort_index()\n            self._is_processed = True\n            return True\n        return False\n    \n    def store(self, stored_df_file):\n        self._df.to_json(stored_df_file)\n        self._df_f.to_json(stored_df_file + '.features')\n        self._df_l.to_json(stored_df_file + '.labels')\n\n    def _filter_empty(self, dfx):\n        return dfx[\n            (dfx[\"uint32 PACKETS\"] &gt; 0) &\n            (dfx[\"uint32 PACKETS_REV\"] &gt; 0) &\n            (dfx[\"uint16* PPI_PKT_LENGTHS\"].str.len() &gt; 0) &\n            (dfx[\"uint16* PPI_PKT_LENGTHS_1\"].str.len() &gt; 0) &\n            (dfx[\"uint16* PPI_PKT_LENGTHS_-1\"].str.len() &gt; 0)\n        ]\n\n    def _norm(self, dfx, fit_new_scaler=True):\n        if self._do_norm:\n            feature_fields_stats = self._feature_fields()\n\n            if fit_new_scaler:\n                scaler = RobustScaler(\n                    unit_variance=False\n                ).fit(dfx[feature_fields_stats])\n                self._scaler = scaler\n            else:\n                scaler = self._scaler\n                assert self._scaler is not None\n\n            dfx[feature_fields_stats] = scaler.transform(dfx[feature_fields_stats])\n        return dfx\n\n    def store_scaler(self, stored_file):\n        dump(self._scaler, stored_file)\n\n    def load_scaler(self, stored_file):\n        self._scaler = load(stored_file)\n\n    def _split_features_label(self, dfx):\n        feature_fields_stats = self._feature_fields()\n\n        dfx_features = dfx[feature_fields_stats].copy()\n        dfx_labels = None\n        if 'IsDoH' in dfx.columns:\n            dfx_labels = dfx[[\"IsDoH\"]].copy()\n        return dfx, dfx_features, dfx_labels\n\n    def process(self, df, stored_df_file, fit_new_scaler=True):\n        if not self.load(stored_df_file):\n            dfx = df.copy()\n\n            dfx = fields_stats(\n                dfx, \n                [\"uint16* PPI_PKT_LENGTHS\", \"uint16* PPI_PKT_LENGTHS_1\", \"uint16* PPI_PKT_LENGTHS_-1\"],\n                self._l_skip,\n                self._l_weights,\n            )\n\n            dfx = fields_stats(\n                dfx, \n                [\"PPI_PKT_INTERVALS\", \"PPI_PKT_INTERVALS_1\", \"PPI_PKT_INTERVALS-1\"],\n                self._t_skip,\n                self._t_weights,\n            )\n\n            dfx = self._filter_empty(dfx)\n            dfx = self._norm(dfx, fit_new_scaler)\n\n            df, df_f, df_l = self._split_features_label(dfx)\n            self._df = df\n            self._df_f = df_f\n            self._df_l = df_l\n            self._is_processed = True\n            self.store(stored_df_file)\n        return self._df, self._df_f, self._df_l"
  },
  {
    "objectID": "appendices2.html#sec-scripts",
    "href": "appendices2.html#sec-scripts",
    "title": "Appendix B — Scripts",
    "section": "B.1 Data collection scripts",
    "text": "B.1 Data collection scripts\n\nB.1.1 Traffic capture tools\nprobe.sh:\nset -x\nservice dnsmasq stop\nservice dnsmasq start\nipfixprobe -i 'pcap;ifc=eth1' -p \"pstats\" -p \"phists\" -p \"tls\" -o 'unirec;i=t:4739:timeout=NO_WAIT:buffer=off:autoflush=off;p=(pstats,phists,tls);v'\nlogger.sh:\n/usr/bin/nemea/logger -i \"t:192.168.1.141:4739\" -t | tee out.csv\n\n\nB.1.2 traffic.py for Experiment 1\ntraffic.py:\nimport os\nimport time\nimport datetime\nimport random\nimport numpy as np\nimport json\n\n# https://moz.com/top-500/download?table=top500Domains\nwith open('sites.txt') as s:\n    sites = [a.strip() for a in s.readlines()]\n\ndoh_providers = [\n    \"https://dns.google/dns-query\",\n    \"https://cloudflare-dns.com/dns-query\",\n    \"https://dns.quad9.net/dns-query\",\n    \"https://unfiltered.adguard-dns.com/dns-query\",\n    \"https://doh.cleanbrowsing.org/doh/security-filter/\",\n    \"https://freedns.controld.com/p0\",\n]\n\ncurrent1 = random.choice(sites)\ncurrent2 = random.choice(sites)\n\na1 = '192.168.2.149:5555' # machine with DoH in Chrome configured\na2 = '192.168.2.249:5555' # control plaintext DNS machine\na0 = '192.168.2.217:5555' # infected machine\n\nos.system(f'adb connect {a0}')\nos.system(f'adb connect {a1}')\nos.system(f'adb connect {a2}')\n\nos.system(f'adb -s {a1} root')\nos.system(f'adb -s {a2} root')\n\ndoh_provider = random.choice(doh_providers)\n\n\ndef clear_dns_cache(host):\n    os.system(f'adb -s {host} shell am force-stop com.android.chrome')\n    os.system(f'adb -s {host} shell ndc resolver clearnetdns wlan0')\n    os.system(f'adb -s {host} shell ndc resolver clearnetdns wifi_eth')\n\n    \ndef change_doh(host, template):\n    os.system(f'adb -s {host} pull \"/data/data/com.android.chrome/app_chrome/Local State\" ./state.json')\n\n    with open(\"state.json\") as f:\n      state = json.load(f)\n\n    state[\"dns_over_https\"] = {\n        \"mode\": \"secure\",\n        \"templates\": template,\n    }\n\n    with open(\"state.json\", \"w\") as f:\n      json.dump(state, f)\n\n    os.system(f'adb -s {host} push ./state.json \"/data/data/com.android.chrome/app_chrome/Local State\"')\n\n\ndef clear_data(host):\n    os.system(f'adb -s {host} shell am force-stop com.android.chrome')\n    os.system(f'adb -s {host} shell pm clear com.android.chrome')\n\n    os.system(f'adb -s {host} shell am set-debug-app --persistent com.android.chrome')\n    os.system(f\"\"\"adb -s {host} shell 'echo \"chrome --disable-fre --no-default-browser-check --no-first-run\" &gt; /data/local/tmp/chrome-command-line'\"\"\")\n    os.system(f'adb -s {host} shell am start -n \"com.android.chrome/com.google.android.apps.chrome.Main\"')\n\n\ndef log(f, text):\n    dt = datetime.datetime.now().strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n    f.write(f\"{dt}: {text}\\n\")\n    print(f\"LOG: {text}\")\n\nbrowsing = False\npages = 0\n\nwith open(\"exp.log\", \"w\") as logf:\n  log(logf, \"Starting\")\n  log(logf, f\"Clear dns cache on {a1} and {a2}\")\n  clear_dns_cache(a1)\n  clear_dns_cache(a2)\n  log(logf, f\"Clear Chrome data for {a1} and {a2}\")\n  clear_data(a1)\n  clear_data(a2)\n\n  time.sleep(5)\n\n  doh_provider = random.choice(doh_providers)\n  log(logf, f\"Change DoH provider on {a1} to {doh_provider}\")\n  change_doh(a1, doh_provider) \n\n  while True:\n    delay = np.random.poisson(4, 1)\n    time.sleep(delay[0])\n    current1 = random.choice(sites)\n    current2 = random.choice(sites)\n    if not browsing:\n        log(logf, \"Continue browsing on {a1} and {a2}\")\n        browsing = True\n    if random.random() &lt; 0.5:\n        log(logf, f\"Open {current1} on {a1}\")\n        os.system(f'adb -s {a1} shell input keyevent 82')\n        os.system(f'adb -s {a1} shell am start -a \"android.intent.action.VIEW\" -d \"https://{current1}\" --es \"com.android.browser.application_id\" \"com.android.browser\"')\n    else:\n        log(logf, f\"Open {current2} on {a2}\")\n        os.system(f'adb -s {a2} shell input keyevent 82')\n        os.system(f'adb -s {a2} shell am start -a \"android.intent.action.VIEW\" -d \"https://{current2}\" --es \"com.android.browser.application_id\" \"com.android.browser\"')\n\n    if random.random() &lt; 0.1:\n        # Clear DNS caches\n        log(logf, f\"Clear dns cache on {a1} and {a2}\")\n        clear_dns_cache(a1)\n        clear_dns_cache(a2)\n        browsing = False\n        pages += 1\n\n    if random.random() &lt; 0.05:\n        clear_dns_cache(a1)\n        clear_dns_cache(a2)\n        # Change DoH provider on Android 2\n        doh_provider = random.choice(doh_providers)\n        log(logf, f\"Change DoH provider on {a1} to {doh_provider}\")\n        change_doh(a1, doh_provider) \n        browsing = False\n\n    if pages &gt; 10:\n        pages = 0\n        log(logf, f\"Clear Chrome data for {a1} and {a2}\")\n        clear_data(a1)\n        clear_data(a2)\n        time.sleep(5)\n        change_doh(a1, doh_provider)\n        browsing = False\n\n\n\nB.1.3 traffic.py for Experiment 2\ntraffic2.py:\nimport os\nimport time\nimport datetime\nimport random\nimport numpy as np\nimport json\n\n# https://moz.com/top-500/download?table=top500Domains\nwith open('sites.txt') as s:\n    sites = [a.strip() for a in s.readlines()]\n\ndoh_providers = [\n    \"https://dns.google/dns-query\",\n    \"https://cloudflare-dns.com/dns-query\",\n    \"https://dns.quad9.net/dns-query\",\n    \"https://unfiltered.adguard-dns.com/dns-query\",\n    \"https://doh.cleanbrowsing.org/doh/security-filter/\",\n    \"https://freedns.controld.com/p0\",\n]\n\ncurrent1 = random.choice(sites)\ncurrent2 = random.choice(sites)\n\na1 = '192.168.2.149:5555' # machine with DoH in Chrome configured\na2 = '192.168.2.217:5555' # infected machine with DoH in Chrome\n\nos.system(f'adb connect {a1}')\nos.system(f'adb connect {a2}')\n\nos.system(f'adb -s {a1} root')\nos.system(f'adb -s {a2} root')\n\ndoh_provider = random.choice(doh_providers)\n\n\ndef clear_dns_cache(host):\n    os.system(f'adb -s {host} shell am force-stop com.android.chrome')\n    os.system(f'adb -s {host} shell ndc resolver clearnetdns wlan0')\n    os.system(f'adb -s {host} shell ndc resolver clearnetdns wifi_eth')\n\n    \ndef change_doh(host, template):\n    os.system(f'adb -s {host} shell am force-stop com.android.chrome')\n    os.system(f'adb -s {host} pull \"/data/data/com.android.chrome/app_chrome/Local State\" ./state.json')\n\n    with open(\"state.json\") as f:\n      state = json.load(f)\n\n    state[\"dns_over_https\"] = {\n        \"mode\": \"secure\",\n        \"templates\": template,\n    }\n\n    with open(\"state.json\", \"w\") as f:\n      json.dump(state, f)\n\n    os.system(f'adb -s {host} push ./state.json \"/data/data/com.android.chrome/app_chrome/Local State\"')\n    os.system(f'adb -s {host} shell am start -n \"com.android.chrome/com.google.android.apps.chrome.Main\"')\n    time.sleep(2)\n\n\ndef clear_data(host):\n    os.system(f'adb -s {host} shell am force-stop com.android.chrome')\n    os.system(f'adb -s {host} shell pm clear com.android.chrome')\n\n    os.system(f'adb -s {host} shell am set-debug-app --persistent com.android.chrome')\n    os.system(f\"\"\"adb -s {host} shell 'echo \"chrome --disable-fre --no-default-browser-check --no-first-run\" &gt; /data/local/tmp/chrome-command-line'\"\"\")\n    os.system(f'adb -s {host} shell am start -n \"com.android.chrome/com.google.android.apps.chrome.Main\"')\n\n\ndef log(f, text):\n    dt = datetime.datetime.now().strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n    f.write(f\"{dt}: {text}\\n\")\n    print(f\"LOG: {text}\")\n\nbrowsing = False\npages = 0\n\nwith open(\"exp.log\", \"w\") as logf:\n  log(logf, \"Starting\")\n  log(logf, f\"Clear dns cache on {a1} and {a2}\")\n  clear_dns_cache(a1)\n  clear_dns_cache(a2)\n  log(logf, f\"Clear Chrome data for {a1} and {a2}\")\n  clear_data(a1)\n  clear_data(a2)\n\n  time.sleep(10)\n\n  doh_provider = random.choice(doh_providers)\n  log(logf, f\"Change DoH provider on {a1} to {doh_provider}\")\n  change_doh(a1, doh_provider) \n  log(logf, f\"Change DoH provider on {a2} to {doh_provider}\")\n  change_doh(a2, doh_provider) \n\n  while True:\n    delay = np.random.poisson(10, 1)\n    time.sleep(delay[0])\n    current1 = random.choice(sites)\n    current2 = random.choice(sites)\n    if not browsing:\n        log(logf, \"Continue browsing on {a1} and {a2}\")\n        browsing = True\n    if random.random() &lt; 0.5:\n        log(logf, f\"Open {current1} on {a1}\")\n        os.system(f'adb -s {a1} shell input keyevent 82')\n        os.system(f'adb -s {a1} shell am start -a \"android.intent.action.VIEW\" -d \"https://{current1}\" --es \"com.android.browser.application_id\" \"com.android.browser\"')\n    else:\n        log(logf, f\"Open {current2} on {a2}\")\n        os.system(f'adb -s {a2} shell input keyevent 82')\n        os.system(f'adb -s {a2} shell am start -a \"android.intent.action.VIEW\" -d \"https://{current2}\" --es \"com.android.browser.application_id\" \"com.android.browser\"')"
  }
]